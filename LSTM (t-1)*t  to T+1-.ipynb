{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score , classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first load the data and take a look at what we have.\n",
    "df = pd.read_csv('heathrow_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], \n",
    "               axis=1,\n",
    "              inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>Hum</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.12</td>\n",
       "      <td>59.75</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>58.67</td>\n",
       "      <td>21.35</td>\n",
       "      <td>27.00</td>\n",
       "      <td>12.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.04</td>\n",
       "      <td>73.87</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.83</td>\n",
       "      <td>25.76</td>\n",
       "      <td>34.37</td>\n",
       "      <td>14.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.00</td>\n",
       "      <td>59.39</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "      <td>65.33</td>\n",
       "      <td>15.23</td>\n",
       "      <td>36.05</td>\n",
       "      <td>20.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.27</td>\n",
       "      <td>68.19</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.00</td>\n",
       "      <td>16.71</td>\n",
       "      <td>42.57</td>\n",
       "      <td>22.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.16</td>\n",
       "      <td>78.65</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>59.58</td>\n",
       "      <td>26.03</td>\n",
       "      <td>38.74</td>\n",
       "      <td>17.279167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>85.24</td>\n",
       "      <td>40.98</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>84.79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>25.35</td>\n",
       "      <td>7.420833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>163.94</td>\n",
       "      <td>37.20</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "      <td>91.63</td>\n",
       "      <td>6.15</td>\n",
       "      <td>27.77</td>\n",
       "      <td>15.304167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>282.06</td>\n",
       "      <td>58.82</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "      <td>94.25</td>\n",
       "      <td>17.17</td>\n",
       "      <td>32.50</td>\n",
       "      <td>13.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>147.20</td>\n",
       "      <td>37.50</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>86.63</td>\n",
       "      <td>8.21</td>\n",
       "      <td>25.78</td>\n",
       "      <td>6.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>74.63</td>\n",
       "      <td>37.61</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "      <td>78.79</td>\n",
       "      <td>8.37</td>\n",
       "      <td>25.99</td>\n",
       "      <td>9.929167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp  wdir  wspd    pres    Hum  \\\n",
       "0       232.12       59.75  18.7  15.5  22.6   0.0     2  12.5  1023.9  58.67   \n",
       "1       168.04       73.87  18.6  13.9  23.4   0.0     1  10.1  1021.8  65.83   \n",
       "2       194.00       59.39  19.2  13.7  24.4   0.0     2   8.4  1021.5  65.33   \n",
       "3       343.27       68.19  20.6  15.7  26.8   0.0     2  10.1  1021.8  65.00   \n",
       "4       190.16       78.65  21.8  14.9  27.8   0.0     3  10.2  1020.0  59.58   \n",
       "..         ...         ...   ...   ...   ...   ...   ...   ...     ...    ...   \n",
       "575      85.24       40.98   3.2   0.1   5.6   0.5     2  11.3  1019.5  84.79   \n",
       "576     163.94       37.20  -0.1  -1.7   2.1   0.0     4  10.6  1018.6  91.63   \n",
       "577     282.06       58.82   0.1  -2.1   2.6   0.0     3   6.6  1026.4  94.25   \n",
       "578     147.20       37.50   4.8  -0.8   8.3   0.5     3  14.8  1020.0  86.63   \n",
       "579      74.63       37.61   5.3   2.2   8.1   5.3     4  12.8  1023.6  78.79   \n",
       "\n",
       "        NO    NO2       PM10  \n",
       "0    21.35  27.00  12.395833  \n",
       "1    25.76  34.37  14.937500  \n",
       "2    15.23  36.05  20.891667  \n",
       "3    16.71  42.57  22.316667  \n",
       "4    26.03  38.74  17.279167  \n",
       "..     ...    ...        ...  \n",
       "575  10.20  25.35   7.420833  \n",
       "576   6.15  27.77  15.304167  \n",
       "577  17.17  32.50  13.537500  \n",
       "578   8.21  25.78   6.412500  \n",
       "579   8.37  25.99   9.929167  \n",
       "\n",
       "[577 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Nox_tropo'], axis=1).values\n",
    "y = df['Nox_tropo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "n_hours = 1\n",
    "n_features = 13\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours , 2)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,14,15,16,17,18,19,20,21,22,23,24,24,25,26]], axis=1, inplace=True)\n",
    "# print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)',\n",
       "       'var6(t-1)', 'var7(t-1)', 'var8(t-1)', 'var9(t-1)', 'var10(t-1)',\n",
       "       'var11(t-1)', 'var12(t-1)', 'var13(t-1)', 'var1(t)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 13) 460 (460,)\n",
      "(460, 1, 13) (460,) (115, 1, 13) (115,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "#80% training data\n",
    "n_train_hours = 460\n",
    "train = values[:n_train_hours]\n",
    "test = values[n_train_hours:]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# train = data.values[:459]\n",
    "# test = data.values[459:]\n",
    "\n",
    "# # Separate input and output\n",
    "# train_X, train_y = train[:, :-1], train[:, -1]\n",
    "# test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# # Reshape input to be 3D [samples, timesteps, features]\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "# # Print all shapes\n",
    "# train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14313806, 0.18170149, 0.14215485, 0.16618872, 0.19475625,\n",
       "       0.13737539, 0.1449406 , 0.10468387, 0.10990031, 0.1327598 ,\n",
       "       0.2962174 , 0.24296054, 0.13729346, 0.3498293 , 0.3001229 ,\n",
       "       0.20092858, 0.20464291, 0.22698348, 0.16657108, 0.19117848,\n",
       "       0.14808139, 0.2341117 , 0.07936638, 0.29602622, 0.22714734,\n",
       "       0.20849379, 0.25874642, 0.10301789, 0.108644  , 0.17866994,\n",
       "       0.201229  , 0.15515499, 0.18735491, 0.16733579, 0.17017616,\n",
       "       0.2211935 , 0.21627748, 0.12265465, 0.1517684 , 0.19554827,\n",
       "       0.26210569, 0.12503073, 0.20546224, 0.16231053, 0.30725113,\n",
       "       0.27783695, 0.35821385, 0.27393145, 0.39912604, 0.17858801,\n",
       "       0.22335109, 0.17823296, 0.27368565, 0.17869726, 0.16501434,\n",
       "       0.18940325, 0.34297419, 0.19309026, 0.16741772, 0.22698348,\n",
       "       0.1809914 , 0.18153762, 0.20294961, 0.14488598, 0.18915745,\n",
       "       0.45647958, 0.58989485, 0.37072238, 0.42138468, 0.3415267 ,\n",
       "       0.50293596, 0.16476854, 0.20297692, 0.10301789, 0.33497201,\n",
       "       0.30132459, 0.35938823, 0.19063225, 0.32322819, 0.30686877,\n",
       "       0.40379626, 0.48586645, 0.10484774, 0.22255906, 0.23383859,\n",
       "       0.26005735, 0.25809095, 0.24888707, 0.57659429, 0.49755565,\n",
       "       0.26033047, 0.34570531, 0.25361191, 0.16195548, 0.14925577,\n",
       "       0.15938823, 0.17998088, 0.26541035, 0.24170422, 0.39038645,\n",
       "       0.31945924, 0.47100915, 0.27095453, 0.06978014, 0.54677045,\n",
       "       0.14682507, 0.25495016, 0.18678137, 0.11541718, 0.25241021,\n",
       "       0.26614775, 0.30449269, 0.29649051, 0.313997  , 0.13939642,\n",
       "       0.43820838, 0.30793391, 0.29236652, 0.26341663, 0.96621603,\n",
       "       0.39139697, 0.37640311, 0.33092995, 0.39792435, 0.20581729,\n",
       "       0.20955892, 0.21272702, 0.32858118, 0.25915608, 0.21854431,\n",
       "       0.27278438, 0.45817288, 0.26278847, 0.13103919, 0.1639492 ,\n",
       "       0.08878875, 0.2353134 , 0.54955619, 0.10886249, 0.26429059,\n",
       "       0.5571214 , 0.38118258, 0.08728663, 0.30949065, 0.2779462 ,\n",
       "       0.17550184, 0.22780281, 0.22911375, 0.6784378 , 0.40464291,\n",
       "       0.39145159, 0.14622423, 0.1213164 , 0.11467978, 0.12718831,\n",
       "       0.21466612, 0.27313942, 0.3193773 , 0.16703537, 0.19991807,\n",
       "       0.16465929, 0.15504575, 0.1965861 , 0.38336747, 0.30583094,\n",
       "       0.07111839, 0.16433156, 0.25077154, 0.23080705, 0.26978014,\n",
       "       0.35037553, 0.07057217, 0.57277072, 0.56648914, 0.34898266,\n",
       "       0.4163321 , 0.73046566, 0.29599891, 0.19199782, 0.05697119,\n",
       "       0.24760344, 0.35671173, 0.57659429, 0.85582412, 0.37653967,\n",
       "       0.14483135, 0.34879148, 0.40944968, 0.26420866, 0.32503073,\n",
       "       0.22064728, 0.21040557, 0.20163867, 0.15903318, 0.36405845,\n",
       "       0.34671583, 0.28816059, 0.79109655, 0.51549911, 0.2830807 ,\n",
       "       0.25606992, 0.28758705, 0.26969821, 0.25232828, 0.29908507,\n",
       "       0.41600437, 0.30727844, 0.79948109, 1.        , 0.86155947,\n",
       "       0.2521371 , 0.19606719, 0.15261505, 0.21674177, 0.18705449,\n",
       "       0.22687423, 0.22695617, 0.12186262, 0.24926943, 0.17214256,\n",
       "       0.15220538, 0.09796531, 0.12309163, 0.05784515, 0.10143384,\n",
       "       0.09788338, 0.14084392, 0.10553052, 0.26860576, 0.25145432,\n",
       "       0.19527516, 0.16561519, 0.23170832, 0.20928581, 0.21133415,\n",
       "       0.29392326, 0.42223133, 0.38650826, 0.35482726, 0.54171788,\n",
       "       0.17684009, 0.03687014, 0.06224225, 0.32251809, 0.30148846,\n",
       "       0.43891848, 0.40904001, 0.27262051, 0.23443944, 0.13863171,\n",
       "       0.24047522, 0.18459648, 0.34633347, 0.56599754, 0.19451045,\n",
       "       0.08302608, 0.13906869, 0.26562884, 0.15307934, 0.23648778,\n",
       "       0.14540489, 0.16790933, 0.07906596, 0.17353544, 0.1919705 ,\n",
       "       0.24976103, 0.13456234, 0.34846374, 0.33994265, 0.34210023,\n",
       "       0.29946743, 0.2730848 , 0.07445036, 0.14764441, 0.08512905,\n",
       "       0.16266557, 0.15182302, 0.20398744, 0.17667623, 0.1371569 ,\n",
       "       0.13041103, 0.12680595, 0.12656015, 0.14321999, 0.14791752,\n",
       "       0.13338796, 0.23124403, 0.25178206, 0.2255906 , 0.19833402,\n",
       "       0.17815103, 0.11508944, 0.09173836, 0.03809914, 0.35548273,\n",
       "       0.13150348, 0.12576813, 0.08351768, 0.22545405, 0.09949474,\n",
       "       0.05765397, 0.15081251, 0.17487369, 0.13865902, 0.06926123,\n",
       "       0.07180117, 0.17785061, 0.10039601, 0.24670217, 0.10301789,\n",
       "       0.10178888, 0.18921207, 0.13581865, 0.18735491, 0.12636897,\n",
       "       0.24763075, 0.1593063 , 0.17836952, 0.2285129 , 0.10370067,\n",
       "       0.37208794, 0.17451864, 0.1727161 , 0.1851427 , 0.05404889,\n",
       "       0.17591151, 0.08499249, 0.14013382, 0.08267104, 0.26000273,\n",
       "       0.1727161 , 0.19617643, 0.18631708, 0.12079749, 0.20051891,\n",
       "       0.21021439, 0.21537621, 0.0867131 , 0.24386181, 0.09690018,\n",
       "       0.15993445, 0.1897583 , 0.18970367, 0.16766353, 0.27753653,\n",
       "       0.22821248, 0.36239246, 0.17460057, 0.14185443, 0.19377304,\n",
       "       0.17815103, 0.0911102 , 0.20505257, 0.18519732, 0.09520688,\n",
       "       0.13625563, 0.18669944, 0.14783559, 0.23550457, 0.14636078,\n",
       "       0.19795166, 0.18334016, 0.19336338, 0.17050389, 0.25544176,\n",
       "       0.18852929, 0.14548682, 0.25241021, 0.28100505, 0.3047385 ,\n",
       "       0.31697392, 0.14018845, 0.29474259, 0.21084255, 0.16547863,\n",
       "       0.20928581, 0.15777687, 0.18129182, 0.18899358, 0.14499522,\n",
       "       0.06439984, 0.20371432, 0.15441759, 0.19907142, 0.26808685,\n",
       "       0.27398607, 0.23252765, 0.30441076, 0.24375256, 0.31500751,\n",
       "       0.22900451, 0.2090127 , 0.40854841, 0.28816059, 0.46336201,\n",
       "       0.32582275, 0.29326779, 0.17470982, 0.26642087, 0.3264236 ,\n",
       "       0.52757067, 0.25393964, 0.3247303 , 0.47557012, 0.25718968,\n",
       "       0.1719787 , 0.19811553, 0.53652875, 0.69534344, 0.36302062,\n",
       "       0.39300833, 0.21280896, 0.14510447, 0.55135873, 0.70105148,\n",
       "       0.12590468, 0.40928581, 0.46565615, 0.47264782, 0.20546224,\n",
       "       0.26117711, 0.17673085, 0.20065547, 0.28835177, 0.23965588,\n",
       "       0.23842687, 0.23747098, 0.40046429, 0.2277755 , 0.18973098,\n",
       "       0.2684419 , 0.15654786, 0.30981838, 0.29862078, 0.24981565,\n",
       "       0.29094633, 0.27592517, 0.13024717, 0.20027311, 0.25440393,\n",
       "       0.21581319, 0.20284037, 0.25888297, 0.23023351, 0.22458009,\n",
       "       0.15157722, 0.26677591, 0.66677591, 0.67849242, 0.47106377,\n",
       "       0.24383449, 0.3705312 , 0.10146115, 0.13969685, 0.20079203,\n",
       "       0.1476171 , 0.14808139, 0.18110064, 0.08813328, 0.10411034])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape[0], train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 - 3s - loss: 0.0146 - val_loss: 0.0270\n",
      "Epoch 2/50\n",
      "154/154 - 0s - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 3/50\n",
      "154/154 - 0s - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 4/50\n",
      "154/154 - 0s - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 5/50\n",
      "154/154 - 0s - loss: 8.8435e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "154/154 - 0s - loss: 7.2948e-04 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "154/154 - 0s - loss: 5.8782e-04 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "154/154 - 1s - loss: 4.9040e-04 - val_loss: 0.0013\n",
      "Epoch 9/50\n",
      "154/154 - 0s - loss: 4.1302e-04 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "154/154 - 0s - loss: 3.5986e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "154/154 - 0s - loss: 3.2157e-04 - val_loss: 0.0011\n",
      "Epoch 12/50\n",
      "154/154 - 0s - loss: 2.8872e-04 - val_loss: 9.8857e-04\n",
      "Epoch 13/50\n",
      "154/154 - 0s - loss: 2.7002e-04 - val_loss: 9.4161e-04\n",
      "Epoch 14/50\n",
      "154/154 - 0s - loss: 2.5286e-04 - val_loss: 8.8238e-04\n",
      "Epoch 15/50\n",
      "154/154 - 0s - loss: 2.3644e-04 - val_loss: 8.1229e-04\n",
      "Epoch 16/50\n",
      "154/154 - 0s - loss: 2.2423e-04 - val_loss: 7.7412e-04\n",
      "Epoch 17/50\n",
      "154/154 - 0s - loss: 2.1072e-04 - val_loss: 7.3273e-04\n",
      "Epoch 18/50\n",
      "154/154 - 0s - loss: 1.9897e-04 - val_loss: 6.8713e-04\n",
      "Epoch 19/50\n",
      "154/154 - 0s - loss: 1.8951e-04 - val_loss: 6.5508e-04\n",
      "Epoch 20/50\n",
      "154/154 - 0s - loss: 1.8131e-04 - val_loss: 6.4071e-04\n",
      "Epoch 21/50\n",
      "154/154 - 0s - loss: 1.7157e-04 - val_loss: 6.5494e-04\n",
      "Epoch 22/50\n",
      "154/154 - 0s - loss: 1.6163e-04 - val_loss: 6.2378e-04\n",
      "Epoch 23/50\n",
      "154/154 - 0s - loss: 1.6150e-04 - val_loss: 6.3657e-04\n",
      "Epoch 24/50\n",
      "154/154 - 0s - loss: 1.5115e-04 - val_loss: 6.1277e-04\n",
      "Epoch 25/50\n",
      "154/154 - 0s - loss: 1.4442e-04 - val_loss: 6.2365e-04\n",
      "Epoch 26/50\n",
      "154/154 - 0s - loss: 1.3779e-04 - val_loss: 5.9162e-04\n",
      "Epoch 27/50\n",
      "154/154 - 0s - loss: 1.3044e-04 - val_loss: 5.6367e-04\n",
      "Epoch 28/50\n",
      "154/154 - 0s - loss: 1.2733e-04 - val_loss: 5.1559e-04\n",
      "Epoch 29/50\n",
      "154/154 - 0s - loss: 1.1937e-04 - val_loss: 4.8008e-04\n",
      "Epoch 30/50\n",
      "154/154 - 0s - loss: 1.1430e-04 - val_loss: 4.5571e-04\n",
      "Epoch 31/50\n",
      "154/154 - 0s - loss: 1.0847e-04 - val_loss: 4.0741e-04\n",
      "Epoch 32/50\n",
      "154/154 - 0s - loss: 1.0672e-04 - val_loss: 3.9876e-04\n",
      "Epoch 33/50\n",
      "154/154 - 0s - loss: 1.0194e-04 - val_loss: 3.3998e-04\n",
      "Epoch 34/50\n",
      "154/154 - 0s - loss: 9.6749e-05 - val_loss: 3.3257e-04\n",
      "Epoch 35/50\n",
      "154/154 - 0s - loss: 9.2191e-05 - val_loss: 3.0098e-04\n",
      "Epoch 36/50\n",
      "154/154 - 0s - loss: 9.6047e-05 - val_loss: 2.7730e-04\n",
      "Epoch 37/50\n",
      "154/154 - 0s - loss: 9.2692e-05 - val_loss: 2.5963e-04\n",
      "Epoch 38/50\n",
      "154/154 - 0s - loss: 8.8172e-05 - val_loss: 2.5012e-04\n",
      "Epoch 39/50\n",
      "154/154 - 0s - loss: 7.9681e-05 - val_loss: 2.0977e-04\n",
      "Epoch 40/50\n",
      "154/154 - 0s - loss: 9.1759e-05 - val_loss: 2.1407e-04\n",
      "Epoch 41/50\n",
      "154/154 - 0s - loss: 7.1392e-05 - val_loss: 1.9347e-04\n",
      "Epoch 42/50\n",
      "154/154 - 0s - loss: 7.4510e-05 - val_loss: 1.7587e-04\n",
      "Epoch 43/50\n",
      "154/154 - 0s - loss: 8.2113e-05 - val_loss: 1.7671e-04\n",
      "Epoch 44/50\n",
      "154/154 - 0s - loss: 7.4799e-05 - val_loss: 1.5991e-04\n",
      "Epoch 45/50\n",
      "154/154 - 0s - loss: 7.7278e-05 - val_loss: 1.5921e-04\n",
      "Epoch 46/50\n",
      "154/154 - 0s - loss: 6.8028e-05 - val_loss: 1.5196e-04\n",
      "Epoch 47/50\n",
      "154/154 - 0s - loss: 6.8866e-05 - val_loss: 1.4743e-04\n",
      "Epoch 48/50\n",
      "154/154 - 0s - loss: 6.1792e-05 - val_loss: 1.4387e-04\n",
      "Epoch 49/50\n",
      "154/154 - 0s - loss: 5.2822e-05 - val_loss: 1.4406e-04\n",
      "Epoch 50/50\n",
      "154/154 - 0s - loss: 5.1595e-05 - val_loss: 1.3535e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(13, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu'))\n",
    "model.add(Dense(38, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 7.453\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -12:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -12:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.387156644438312"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.783642046063335"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13-38-1 Test RMSE: 62.006\n",
    "\n",
    "13-68-1 Test RMSE: 68.752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 61-1 Test RMSE: Test RMSE: 63.163\n",
    "\n",
    "13-40-1 Test RMSE: 67.366\n",
    "\n",
    "13-100-1 Test RMSE: 65.965\n",
    "\n",
    "13- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
