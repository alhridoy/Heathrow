{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score , classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first load the data and take a look at what we have.\n",
    "df = pd.read_csv('heathrow_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], \n",
    "               axis=1,\n",
    "              inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>Hum</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.12</td>\n",
       "      <td>59.75</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>58.67</td>\n",
       "      <td>21.35</td>\n",
       "      <td>27.00</td>\n",
       "      <td>12.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.04</td>\n",
       "      <td>73.87</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.83</td>\n",
       "      <td>25.76</td>\n",
       "      <td>34.37</td>\n",
       "      <td>14.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.00</td>\n",
       "      <td>59.39</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "      <td>65.33</td>\n",
       "      <td>15.23</td>\n",
       "      <td>36.05</td>\n",
       "      <td>20.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.27</td>\n",
       "      <td>68.19</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.00</td>\n",
       "      <td>16.71</td>\n",
       "      <td>42.57</td>\n",
       "      <td>22.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.16</td>\n",
       "      <td>78.65</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>59.58</td>\n",
       "      <td>26.03</td>\n",
       "      <td>38.74</td>\n",
       "      <td>17.279167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>85.24</td>\n",
       "      <td>40.98</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>84.79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>25.35</td>\n",
       "      <td>7.420833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>163.94</td>\n",
       "      <td>37.20</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "      <td>91.63</td>\n",
       "      <td>6.15</td>\n",
       "      <td>27.77</td>\n",
       "      <td>15.304167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>282.06</td>\n",
       "      <td>58.82</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "      <td>94.25</td>\n",
       "      <td>17.17</td>\n",
       "      <td>32.50</td>\n",
       "      <td>13.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>147.20</td>\n",
       "      <td>37.50</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>86.63</td>\n",
       "      <td>8.21</td>\n",
       "      <td>25.78</td>\n",
       "      <td>6.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>74.63</td>\n",
       "      <td>37.61</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "      <td>78.79</td>\n",
       "      <td>8.37</td>\n",
       "      <td>25.99</td>\n",
       "      <td>9.929167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp  wdir  wspd    pres    Hum  \\\n",
       "0       232.12       59.75  18.7  15.5  22.6   0.0     2  12.5  1023.9  58.67   \n",
       "1       168.04       73.87  18.6  13.9  23.4   0.0     1  10.1  1021.8  65.83   \n",
       "2       194.00       59.39  19.2  13.7  24.4   0.0     2   8.4  1021.5  65.33   \n",
       "3       343.27       68.19  20.6  15.7  26.8   0.0     2  10.1  1021.8  65.00   \n",
       "4       190.16       78.65  21.8  14.9  27.8   0.0     3  10.2  1020.0  59.58   \n",
       "..         ...         ...   ...   ...   ...   ...   ...   ...     ...    ...   \n",
       "575      85.24       40.98   3.2   0.1   5.6   0.5     2  11.3  1019.5  84.79   \n",
       "576     163.94       37.20  -0.1  -1.7   2.1   0.0     4  10.6  1018.6  91.63   \n",
       "577     282.06       58.82   0.1  -2.1   2.6   0.0     3   6.6  1026.4  94.25   \n",
       "578     147.20       37.50   4.8  -0.8   8.3   0.5     3  14.8  1020.0  86.63   \n",
       "579      74.63       37.61   5.3   2.2   8.1   5.3     4  12.8  1023.6  78.79   \n",
       "\n",
       "        NO    NO2       PM10  \n",
       "0    21.35  27.00  12.395833  \n",
       "1    25.76  34.37  14.937500  \n",
       "2    15.23  36.05  20.891667  \n",
       "3    16.71  42.57  22.316667  \n",
       "4    26.03  38.74  17.279167  \n",
       "..     ...    ...        ...  \n",
       "575  10.20  25.35   7.420833  \n",
       "576   6.15  27.77  15.304167  \n",
       "577  17.17  32.50  13.537500  \n",
       "578   8.21  25.78   6.412500  \n",
       "579   8.37  25.99   9.929167  \n",
       "\n",
       "[577 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Nox_tropo'], axis=1).values\n",
    "y = df['Nox_tropo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "n_hours = 1\n",
    "n_features = 13\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours , 7)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,\n",
    "                               52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90]], axis=1, inplace=True)\n",
    "# print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)',\n",
       "       'var6(t-1)', 'var7(t-1)', 'var8(t-1)', 'var9(t-1)', 'var10(t-1)',\n",
       "       'var11(t-1)', 'var12(t-1)', 'var13(t-1)', 'var1(t+6)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 13) 460 (460,)\n",
      "(460, 1, 13) (460,) (110, 1, 13) (110,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "#80% training data\n",
    "n_train_hours = 460\n",
    "train = values[:n_train_hours]\n",
    "test = values[n_train_hours:]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# train = data.values[:459]\n",
    "# test = data.values[459:]\n",
    "\n",
    "# # Separate input and output\n",
    "# train_X, train_y = train[:, :-1], train[:, -1]\n",
    "# test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# # Reshape input to be 3D [samples, timesteps, features]\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "# # Print all shapes\n",
    "# train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14313806, 0.18170149, 0.14215485, 0.16618872, 0.19475625,\n",
       "       0.13737539, 0.1449406 , 0.10468387, 0.10990031, 0.1327598 ,\n",
       "       0.2962174 , 0.24296054, 0.13729346, 0.3498293 , 0.3001229 ,\n",
       "       0.20092858, 0.20464291, 0.22698348, 0.16657108, 0.19117848,\n",
       "       0.14808139, 0.2341117 , 0.07936638, 0.29602622, 0.22714734,\n",
       "       0.20849379, 0.25874642, 0.10301789, 0.108644  , 0.17866994,\n",
       "       0.201229  , 0.15515499, 0.18735491, 0.16733579, 0.17017616,\n",
       "       0.2211935 , 0.21627748, 0.12265465, 0.1517684 , 0.19554827,\n",
       "       0.26210569, 0.12503073, 0.20546224, 0.16231053, 0.30725113,\n",
       "       0.27783695, 0.35821385, 0.27393145, 0.39912604, 0.17858801,\n",
       "       0.22335109, 0.17823296, 0.27368565, 0.17869726, 0.16501434,\n",
       "       0.18940325, 0.34297419, 0.19309026, 0.16741772, 0.22698348,\n",
       "       0.1809914 , 0.18153762, 0.20294961, 0.14488598, 0.18915745,\n",
       "       0.45647958, 0.58989485, 0.37072238, 0.42138468, 0.3415267 ,\n",
       "       0.50293596, 0.16476854, 0.20297692, 0.10301789, 0.33497201,\n",
       "       0.30132459, 0.35938823, 0.19063225, 0.32322819, 0.30686877,\n",
       "       0.40379626, 0.48586645, 0.10484774, 0.22255906, 0.23383859,\n",
       "       0.26005735, 0.25809095, 0.24888707, 0.57659429, 0.49755565,\n",
       "       0.26033047, 0.34570531, 0.25361191, 0.16195548, 0.14925577,\n",
       "       0.15938823, 0.17998088, 0.26541035, 0.24170422, 0.39038645,\n",
       "       0.31945924, 0.47100915, 0.27095453, 0.06978014, 0.54677045,\n",
       "       0.14682507, 0.25495016, 0.18678137, 0.11541718, 0.25241021,\n",
       "       0.26614775, 0.30449269, 0.29649051, 0.313997  , 0.13939642,\n",
       "       0.43820838, 0.30793391, 0.29236652, 0.26341663, 0.96621603,\n",
       "       0.39139697, 0.37640311, 0.33092995, 0.39792435, 0.20581729,\n",
       "       0.20955892, 0.21272702, 0.32858118, 0.25915608, 0.21854431,\n",
       "       0.27278438, 0.45817288, 0.26278847, 0.13103919, 0.1639492 ,\n",
       "       0.08878875, 0.2353134 , 0.54955619, 0.10886249, 0.26429059,\n",
       "       0.5571214 , 0.38118258, 0.08728663, 0.30949065, 0.2779462 ,\n",
       "       0.17550184, 0.22780281, 0.22911375, 0.6784378 , 0.40464291,\n",
       "       0.39145159, 0.14622423, 0.1213164 , 0.11467978, 0.12718831,\n",
       "       0.21466612, 0.27313942, 0.3193773 , 0.16703537, 0.19991807,\n",
       "       0.16465929, 0.15504575, 0.1965861 , 0.38336747, 0.30583094,\n",
       "       0.07111839, 0.16433156, 0.25077154, 0.23080705, 0.26978014,\n",
       "       0.35037553, 0.07057217, 0.57277072, 0.56648914, 0.34898266,\n",
       "       0.4163321 , 0.73046566, 0.29599891, 0.19199782, 0.05697119,\n",
       "       0.24760344, 0.35671173, 0.57659429, 0.85582412, 0.37653967,\n",
       "       0.14483135, 0.34879148, 0.40944968, 0.26420866, 0.32503073,\n",
       "       0.22064728, 0.21040557, 0.20163867, 0.15903318, 0.36405845,\n",
       "       0.34671583, 0.28816059, 0.79109655, 0.51549911, 0.2830807 ,\n",
       "       0.25606992, 0.28758705, 0.26969821, 0.25232828, 0.29908507,\n",
       "       0.41600437, 0.30727844, 0.79948109, 1.        , 0.86155947,\n",
       "       0.2521371 , 0.19606719, 0.15261505, 0.21674177, 0.18705449,\n",
       "       0.22687423, 0.22695617, 0.12186262, 0.24926943, 0.17214256,\n",
       "       0.15220538, 0.09796531, 0.12309163, 0.05784515, 0.10143384,\n",
       "       0.09788338, 0.14084392, 0.10553052, 0.26860576, 0.25145432,\n",
       "       0.19527516, 0.16561519, 0.23170832, 0.20928581, 0.21133415,\n",
       "       0.29392326, 0.42223133, 0.38650826, 0.35482726, 0.54171788,\n",
       "       0.17684009, 0.03687014, 0.06224225, 0.32251809, 0.30148846,\n",
       "       0.43891848, 0.40904001, 0.27262051, 0.23443944, 0.13863171,\n",
       "       0.24047522, 0.18459648, 0.34633347, 0.56599754, 0.19451045,\n",
       "       0.08302608, 0.13906869, 0.26562884, 0.15307934, 0.23648778,\n",
       "       0.14540489, 0.16790933, 0.07906596, 0.17353544, 0.1919705 ,\n",
       "       0.24976103, 0.13456234, 0.34846374, 0.33994265, 0.34210023,\n",
       "       0.29946743, 0.2730848 , 0.07445036, 0.14764441, 0.08512905,\n",
       "       0.16266557, 0.15182302, 0.20398744, 0.17667623, 0.1371569 ,\n",
       "       0.13041103, 0.12680595, 0.12656015, 0.14321999, 0.14791752,\n",
       "       0.13338796, 0.23124403, 0.25178206, 0.2255906 , 0.19833402,\n",
       "       0.17815103, 0.11508944, 0.09173836, 0.03809914, 0.35548273,\n",
       "       0.13150348, 0.12576813, 0.08351768, 0.22545405, 0.09949474,\n",
       "       0.05765397, 0.15081251, 0.17487369, 0.13865902, 0.06926123,\n",
       "       0.07180117, 0.17785061, 0.10039601, 0.24670217, 0.10301789,\n",
       "       0.10178888, 0.18921207, 0.13581865, 0.18735491, 0.12636897,\n",
       "       0.24763075, 0.1593063 , 0.17836952, 0.2285129 , 0.10370067,\n",
       "       0.37208794, 0.17451864, 0.1727161 , 0.1851427 , 0.05404889,\n",
       "       0.17591151, 0.08499249, 0.14013382, 0.08267104, 0.26000273,\n",
       "       0.1727161 , 0.19617643, 0.18631708, 0.12079749, 0.20051891,\n",
       "       0.21021439, 0.21537621, 0.0867131 , 0.24386181, 0.09690018,\n",
       "       0.15993445, 0.1897583 , 0.18970367, 0.16766353, 0.27753653,\n",
       "       0.22821248, 0.36239246, 0.17460057, 0.14185443, 0.19377304,\n",
       "       0.17815103, 0.0911102 , 0.20505257, 0.18519732, 0.09520688,\n",
       "       0.13625563, 0.18669944, 0.14783559, 0.23550457, 0.14636078,\n",
       "       0.19795166, 0.18334016, 0.19336338, 0.17050389, 0.25544176,\n",
       "       0.18852929, 0.14548682, 0.25241021, 0.28100505, 0.3047385 ,\n",
       "       0.31697392, 0.14018845, 0.29474259, 0.21084255, 0.16547863,\n",
       "       0.20928581, 0.15777687, 0.18129182, 0.18899358, 0.14499522,\n",
       "       0.06439984, 0.20371432, 0.15441759, 0.19907142, 0.26808685,\n",
       "       0.27398607, 0.23252765, 0.30441076, 0.24375256, 0.31500751,\n",
       "       0.22900451, 0.2090127 , 0.40854841, 0.28816059, 0.46336201,\n",
       "       0.32582275, 0.29326779, 0.17470982, 0.26642087, 0.3264236 ,\n",
       "       0.52757067, 0.25393964, 0.3247303 , 0.47557012, 0.25718968,\n",
       "       0.1719787 , 0.19811553, 0.53652875, 0.69534344, 0.36302062,\n",
       "       0.39300833, 0.21280896, 0.14510447, 0.55135873, 0.70105148,\n",
       "       0.12590468, 0.40928581, 0.46565615, 0.47264782, 0.20546224,\n",
       "       0.26117711, 0.17673085, 0.20065547, 0.28835177, 0.23965588,\n",
       "       0.23842687, 0.23747098, 0.40046429, 0.2277755 , 0.18973098,\n",
       "       0.2684419 , 0.15654786, 0.30981838, 0.29862078, 0.24981565,\n",
       "       0.29094633, 0.27592517, 0.13024717, 0.20027311, 0.25440393,\n",
       "       0.21581319, 0.20284037, 0.25888297, 0.23023351, 0.22458009,\n",
       "       0.15157722, 0.26677591, 0.66677591, 0.67849242, 0.47106377,\n",
       "       0.24383449, 0.3705312 , 0.10146115, 0.13969685, 0.20079203,\n",
       "       0.1476171 , 0.14808139, 0.18110064, 0.08813328, 0.10411034])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 - 3s - loss: 0.0146 - val_loss: 0.0269\n",
      "Epoch 2/50\n",
      "154/154 - 0s - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 3/50\n",
      "154/154 - 0s - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 4/50\n",
      "154/154 - 0s - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 5/50\n",
      "154/154 - 0s - loss: 8.8435e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "154/154 - 0s - loss: 7.2948e-04 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "154/154 - 0s - loss: 5.8782e-04 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "154/154 - 0s - loss: 4.9040e-04 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "154/154 - 0s - loss: 4.1302e-04 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "154/154 - 0s - loss: 3.5986e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "154/154 - 0s - loss: 3.2157e-04 - val_loss: 0.0011\n",
      "Epoch 12/50\n",
      "154/154 - 0s - loss: 2.8872e-04 - val_loss: 9.8954e-04\n",
      "Epoch 13/50\n",
      "154/154 - 0s - loss: 2.7002e-04 - val_loss: 9.4079e-04\n",
      "Epoch 14/50\n",
      "154/154 - 0s - loss: 2.5286e-04 - val_loss: 8.8181e-04\n",
      "Epoch 15/50\n",
      "154/154 - 0s - loss: 2.3644e-04 - val_loss: 8.1193e-04\n",
      "Epoch 16/50\n",
      "154/154 - 0s - loss: 2.2423e-04 - val_loss: 7.7174e-04\n",
      "Epoch 17/50\n",
      "154/154 - 0s - loss: 2.1072e-04 - val_loss: 7.3022e-04\n",
      "Epoch 18/50\n",
      "154/154 - 0s - loss: 1.9897e-04 - val_loss: 6.8167e-04\n",
      "Epoch 19/50\n",
      "154/154 - 0s - loss: 1.8951e-04 - val_loss: 6.4820e-04\n",
      "Epoch 20/50\n",
      "154/154 - 0s - loss: 1.8131e-04 - val_loss: 6.3008e-04\n",
      "Epoch 21/50\n",
      "154/154 - 0s - loss: 1.7157e-04 - val_loss: 6.4313e-04\n",
      "Epoch 22/50\n",
      "154/154 - 0s - loss: 1.6163e-04 - val_loss: 6.1099e-04\n",
      "Epoch 23/50\n",
      "154/154 - 0s - loss: 1.6150e-04 - val_loss: 6.1999e-04\n",
      "Epoch 24/50\n",
      "154/154 - 0s - loss: 1.5115e-04 - val_loss: 5.9770e-04\n",
      "Epoch 25/50\n",
      "154/154 - 0s - loss: 1.4442e-04 - val_loss: 6.0907e-04\n",
      "Epoch 26/50\n",
      "154/154 - 0s - loss: 1.3779e-04 - val_loss: 5.7703e-04\n",
      "Epoch 27/50\n",
      "154/154 - 0s - loss: 1.3044e-04 - val_loss: 5.5014e-04\n",
      "Epoch 28/50\n",
      "154/154 - 0s - loss: 1.2733e-04 - val_loss: 5.0466e-04\n",
      "Epoch 29/50\n",
      "154/154 - 0s - loss: 1.1937e-04 - val_loss: 4.6866e-04\n",
      "Epoch 30/50\n",
      "154/154 - 0s - loss: 1.1430e-04 - val_loss: 4.4548e-04\n",
      "Epoch 31/50\n",
      "154/154 - 0s - loss: 1.0847e-04 - val_loss: 3.9718e-04\n",
      "Epoch 32/50\n",
      "154/154 - 0s - loss: 1.0672e-04 - val_loss: 3.8883e-04\n",
      "Epoch 33/50\n",
      "154/154 - 0s - loss: 1.0194e-04 - val_loss: 3.3118e-04\n",
      "Epoch 34/50\n",
      "154/154 - 0s - loss: 9.6749e-05 - val_loss: 3.2383e-04\n",
      "Epoch 35/50\n",
      "154/154 - 0s - loss: 9.2191e-05 - val_loss: 2.9379e-04\n",
      "Epoch 36/50\n",
      "154/154 - 0s - loss: 9.6047e-05 - val_loss: 2.7028e-04\n",
      "Epoch 37/50\n",
      "154/154 - 0s - loss: 9.2692e-05 - val_loss: 2.5356e-04\n",
      "Epoch 38/50\n",
      "154/154 - 0s - loss: 8.8172e-05 - val_loss: 2.4390e-04\n",
      "Epoch 39/50\n",
      "154/154 - 0s - loss: 7.9681e-05 - val_loss: 2.0564e-04\n",
      "Epoch 40/50\n",
      "154/154 - 0s - loss: 9.1759e-05 - val_loss: 2.1003e-04\n",
      "Epoch 41/50\n",
      "154/154 - 0s - loss: 7.1392e-05 - val_loss: 1.8925e-04\n",
      "Epoch 42/50\n",
      "154/154 - 0s - loss: 7.4510e-05 - val_loss: 1.7403e-04\n",
      "Epoch 43/50\n",
      "154/154 - 0s - loss: 8.2113e-05 - val_loss: 1.7224e-04\n",
      "Epoch 44/50\n",
      "154/154 - 0s - loss: 7.4799e-05 - val_loss: 1.5766e-04\n",
      "Epoch 45/50\n",
      "154/154 - 0s - loss: 7.7278e-05 - val_loss: 1.5648e-04\n",
      "Epoch 46/50\n",
      "154/154 - 0s - loss: 6.8028e-05 - val_loss: 1.5008e-04\n",
      "Epoch 47/50\n",
      "154/154 - 0s - loss: 6.8866e-05 - val_loss: 1.4558e-04\n",
      "Epoch 48/50\n",
      "154/154 - 0s - loss: 6.1792e-05 - val_loss: 1.4231e-04\n",
      "Epoch 49/50\n",
      "154/154 - 0s - loss: 5.2822e-05 - val_loss: 1.4253e-04\n",
      "Epoch 50/50\n",
      "154/154 - 0s - loss: 5.1595e-05 - val_loss: 1.3492e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(13, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu'))\n",
    "model.add(Dense(38, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 7.441\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -12:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -12:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.354929024738783"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.593089418515323"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13-38-1 Test RMSE: 62.006\n",
    "\n",
    "13-68-1 Test RMSE: 68.752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 61-1 Test RMSE: Test RMSE: 63.163\n",
    "\n",
    "13-40-1 Test RMSE: 67.366\n",
    "\n",
    "13-100-1 Test RMSE: 65.965\n",
    "\n",
    "13- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
