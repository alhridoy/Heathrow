{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score , classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first load the data and take a look at what we have.\n",
    "df = pd.read_csv('heathrow_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], \n",
    "               axis=1,\n",
    "              inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>Hum</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.12</td>\n",
       "      <td>59.75</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>58.67</td>\n",
       "      <td>21.35</td>\n",
       "      <td>27.00</td>\n",
       "      <td>12.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.04</td>\n",
       "      <td>73.87</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.83</td>\n",
       "      <td>25.76</td>\n",
       "      <td>34.37</td>\n",
       "      <td>14.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.00</td>\n",
       "      <td>59.39</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "      <td>65.33</td>\n",
       "      <td>15.23</td>\n",
       "      <td>36.05</td>\n",
       "      <td>20.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.27</td>\n",
       "      <td>68.19</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.00</td>\n",
       "      <td>16.71</td>\n",
       "      <td>42.57</td>\n",
       "      <td>22.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.16</td>\n",
       "      <td>78.65</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>59.58</td>\n",
       "      <td>26.03</td>\n",
       "      <td>38.74</td>\n",
       "      <td>17.279167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>85.24</td>\n",
       "      <td>40.98</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>84.79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>25.35</td>\n",
       "      <td>7.420833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>163.94</td>\n",
       "      <td>37.20</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "      <td>91.63</td>\n",
       "      <td>6.15</td>\n",
       "      <td>27.77</td>\n",
       "      <td>15.304167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>282.06</td>\n",
       "      <td>58.82</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "      <td>94.25</td>\n",
       "      <td>17.17</td>\n",
       "      <td>32.50</td>\n",
       "      <td>13.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>147.20</td>\n",
       "      <td>37.50</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>86.63</td>\n",
       "      <td>8.21</td>\n",
       "      <td>25.78</td>\n",
       "      <td>6.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>74.63</td>\n",
       "      <td>37.61</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "      <td>78.79</td>\n",
       "      <td>8.37</td>\n",
       "      <td>25.99</td>\n",
       "      <td>9.929167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp  wdir  wspd    pres    Hum  \\\n",
       "0       232.12       59.75  18.7  15.5  22.6   0.0     2  12.5  1023.9  58.67   \n",
       "1       168.04       73.87  18.6  13.9  23.4   0.0     1  10.1  1021.8  65.83   \n",
       "2       194.00       59.39  19.2  13.7  24.4   0.0     2   8.4  1021.5  65.33   \n",
       "3       343.27       68.19  20.6  15.7  26.8   0.0     2  10.1  1021.8  65.00   \n",
       "4       190.16       78.65  21.8  14.9  27.8   0.0     3  10.2  1020.0  59.58   \n",
       "..         ...         ...   ...   ...   ...   ...   ...   ...     ...    ...   \n",
       "575      85.24       40.98   3.2   0.1   5.6   0.5     2  11.3  1019.5  84.79   \n",
       "576     163.94       37.20  -0.1  -1.7   2.1   0.0     4  10.6  1018.6  91.63   \n",
       "577     282.06       58.82   0.1  -2.1   2.6   0.0     3   6.6  1026.4  94.25   \n",
       "578     147.20       37.50   4.8  -0.8   8.3   0.5     3  14.8  1020.0  86.63   \n",
       "579      74.63       37.61   5.3   2.2   8.1   5.3     4  12.8  1023.6  78.79   \n",
       "\n",
       "        NO    NO2       PM10  \n",
       "0    21.35  27.00  12.395833  \n",
       "1    25.76  34.37  14.937500  \n",
       "2    15.23  36.05  20.891667  \n",
       "3    16.71  42.57  22.316667  \n",
       "4    26.03  38.74  17.279167  \n",
       "..     ...    ...        ...  \n",
       "575  10.20  25.35   7.420833  \n",
       "576   6.15  27.77  15.304167  \n",
       "577  17.17  32.50  13.537500  \n",
       "578   8.21  25.78   6.412500  \n",
       "579   8.37  25.99   9.929167  \n",
       "\n",
       "[577 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Nox_tropo'], axis=1).values\n",
    "y = df['Nox_tropo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "n_hours = 1\n",
    "n_features = 13\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours , 5)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,\n",
    "                               52,53,54,55,56,57,58,59,60,61,62,63,64]], axis=1, inplace=True)\n",
    "# print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)',\n",
       "       'var6(t-1)', 'var7(t-1)', 'var8(t-1)', 'var9(t-1)', 'var10(t-1)',\n",
       "       'var11(t-1)', 'var12(t-1)', 'var13(t-1)', 'var1(t+4)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 13) 460 (460,)\n",
      "(460, 1, 13) (460,) (112, 1, 13) (112,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "#80% training data\n",
    "n_train_hours = 460\n",
    "train = values[:n_train_hours]\n",
    "test = values[n_train_hours:]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# train = data.values[:459]\n",
    "# test = data.values[459:]\n",
    "\n",
    "# # Separate input and output\n",
    "# train_X, train_y = train[:, :-1], train[:, -1]\n",
    "# test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# # Reshape input to be 3D [samples, timesteps, features]\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "# # Print all shapes\n",
    "# train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14313806, 0.18170149, 0.14215485, 0.16618872, 0.19475625,\n",
       "       0.13737539, 0.1449406 , 0.10468387, 0.10990031, 0.1327598 ,\n",
       "       0.2962174 , 0.24296054, 0.13729346, 0.3498293 , 0.3001229 ,\n",
       "       0.20092858, 0.20464291, 0.22698348, 0.16657108, 0.19117848,\n",
       "       0.14808139, 0.2341117 , 0.07936638, 0.29602622, 0.22714734,\n",
       "       0.20849379, 0.25874642, 0.10301789, 0.108644  , 0.17866994,\n",
       "       0.201229  , 0.15515499, 0.18735491, 0.16733579, 0.17017616,\n",
       "       0.2211935 , 0.21627748, 0.12265465, 0.1517684 , 0.19554827,\n",
       "       0.26210569, 0.12503073, 0.20546224, 0.16231053, 0.30725113,\n",
       "       0.27783695, 0.35821385, 0.27393145, 0.39912604, 0.17858801,\n",
       "       0.22335109, 0.17823296, 0.27368565, 0.17869726, 0.16501434,\n",
       "       0.18940325, 0.34297419, 0.19309026, 0.16741772, 0.22698348,\n",
       "       0.1809914 , 0.18153762, 0.20294961, 0.14488598, 0.18915745,\n",
       "       0.45647958, 0.58989485, 0.37072238, 0.42138468, 0.3415267 ,\n",
       "       0.50293596, 0.16476854, 0.20297692, 0.10301789, 0.33497201,\n",
       "       0.30132459, 0.35938823, 0.19063225, 0.32322819, 0.30686877,\n",
       "       0.40379626, 0.48586645, 0.10484774, 0.22255906, 0.23383859,\n",
       "       0.26005735, 0.25809095, 0.24888707, 0.57659429, 0.49755565,\n",
       "       0.26033047, 0.34570531, 0.25361191, 0.16195548, 0.14925577,\n",
       "       0.15938823, 0.17998088, 0.26541035, 0.24170422, 0.39038645,\n",
       "       0.31945924, 0.47100915, 0.27095453, 0.06978014, 0.54677045,\n",
       "       0.14682507, 0.25495016, 0.18678137, 0.11541718, 0.25241021,\n",
       "       0.26614775, 0.30449269, 0.29649051, 0.313997  , 0.13939642,\n",
       "       0.43820838, 0.30793391, 0.29236652, 0.26341663, 0.96621603,\n",
       "       0.39139697, 0.37640311, 0.33092995, 0.39792435, 0.20581729,\n",
       "       0.20955892, 0.21272702, 0.32858118, 0.25915608, 0.21854431,\n",
       "       0.27278438, 0.45817288, 0.26278847, 0.13103919, 0.1639492 ,\n",
       "       0.08878875, 0.2353134 , 0.54955619, 0.10886249, 0.26429059,\n",
       "       0.5571214 , 0.38118258, 0.08728663, 0.30949065, 0.2779462 ,\n",
       "       0.17550184, 0.22780281, 0.22911375, 0.6784378 , 0.40464291,\n",
       "       0.39145159, 0.14622423, 0.1213164 , 0.11467978, 0.12718831,\n",
       "       0.21466612, 0.27313942, 0.3193773 , 0.16703537, 0.19991807,\n",
       "       0.16465929, 0.15504575, 0.1965861 , 0.38336747, 0.30583094,\n",
       "       0.07111839, 0.16433156, 0.25077154, 0.23080705, 0.26978014,\n",
       "       0.35037553, 0.07057217, 0.57277072, 0.56648914, 0.34898266,\n",
       "       0.4163321 , 0.73046566, 0.29599891, 0.19199782, 0.05697119,\n",
       "       0.24760344, 0.35671173, 0.57659429, 0.85582412, 0.37653967,\n",
       "       0.14483135, 0.34879148, 0.40944968, 0.26420866, 0.32503073,\n",
       "       0.22064728, 0.21040557, 0.20163867, 0.15903318, 0.36405845,\n",
       "       0.34671583, 0.28816059, 0.79109655, 0.51549911, 0.2830807 ,\n",
       "       0.25606992, 0.28758705, 0.26969821, 0.25232828, 0.29908507,\n",
       "       0.41600437, 0.30727844, 0.79948109, 1.        , 0.86155947,\n",
       "       0.2521371 , 0.19606719, 0.15261505, 0.21674177, 0.18705449,\n",
       "       0.22687423, 0.22695617, 0.12186262, 0.24926943, 0.17214256,\n",
       "       0.15220538, 0.09796531, 0.12309163, 0.05784515, 0.10143384,\n",
       "       0.09788338, 0.14084392, 0.10553052, 0.26860576, 0.25145432,\n",
       "       0.19527516, 0.16561519, 0.23170832, 0.20928581, 0.21133415,\n",
       "       0.29392326, 0.42223133, 0.38650826, 0.35482726, 0.54171788,\n",
       "       0.17684009, 0.03687014, 0.06224225, 0.32251809, 0.30148846,\n",
       "       0.43891848, 0.40904001, 0.27262051, 0.23443944, 0.13863171,\n",
       "       0.24047522, 0.18459648, 0.34633347, 0.56599754, 0.19451045,\n",
       "       0.08302608, 0.13906869, 0.26562884, 0.15307934, 0.23648778,\n",
       "       0.14540489, 0.16790933, 0.07906596, 0.17353544, 0.1919705 ,\n",
       "       0.24976103, 0.13456234, 0.34846374, 0.33994265, 0.34210023,\n",
       "       0.29946743, 0.2730848 , 0.07445036, 0.14764441, 0.08512905,\n",
       "       0.16266557, 0.15182302, 0.20398744, 0.17667623, 0.1371569 ,\n",
       "       0.13041103, 0.12680595, 0.12656015, 0.14321999, 0.14791752,\n",
       "       0.13338796, 0.23124403, 0.25178206, 0.2255906 , 0.19833402,\n",
       "       0.17815103, 0.11508944, 0.09173836, 0.03809914, 0.35548273,\n",
       "       0.13150348, 0.12576813, 0.08351768, 0.22545405, 0.09949474,\n",
       "       0.05765397, 0.15081251, 0.17487369, 0.13865902, 0.06926123,\n",
       "       0.07180117, 0.17785061, 0.10039601, 0.24670217, 0.10301789,\n",
       "       0.10178888, 0.18921207, 0.13581865, 0.18735491, 0.12636897,\n",
       "       0.24763075, 0.1593063 , 0.17836952, 0.2285129 , 0.10370067,\n",
       "       0.37208794, 0.17451864, 0.1727161 , 0.1851427 , 0.05404889,\n",
       "       0.17591151, 0.08499249, 0.14013382, 0.08267104, 0.26000273,\n",
       "       0.1727161 , 0.19617643, 0.18631708, 0.12079749, 0.20051891,\n",
       "       0.21021439, 0.21537621, 0.0867131 , 0.24386181, 0.09690018,\n",
       "       0.15993445, 0.1897583 , 0.18970367, 0.16766353, 0.27753653,\n",
       "       0.22821248, 0.36239246, 0.17460057, 0.14185443, 0.19377304,\n",
       "       0.17815103, 0.0911102 , 0.20505257, 0.18519732, 0.09520688,\n",
       "       0.13625563, 0.18669944, 0.14783559, 0.23550457, 0.14636078,\n",
       "       0.19795166, 0.18334016, 0.19336338, 0.17050389, 0.25544176,\n",
       "       0.18852929, 0.14548682, 0.25241021, 0.28100505, 0.3047385 ,\n",
       "       0.31697392, 0.14018845, 0.29474259, 0.21084255, 0.16547863,\n",
       "       0.20928581, 0.15777687, 0.18129182, 0.18899358, 0.14499522,\n",
       "       0.06439984, 0.20371432, 0.15441759, 0.19907142, 0.26808685,\n",
       "       0.27398607, 0.23252765, 0.30441076, 0.24375256, 0.31500751,\n",
       "       0.22900451, 0.2090127 , 0.40854841, 0.28816059, 0.46336201,\n",
       "       0.32582275, 0.29326779, 0.17470982, 0.26642087, 0.3264236 ,\n",
       "       0.52757067, 0.25393964, 0.3247303 , 0.47557012, 0.25718968,\n",
       "       0.1719787 , 0.19811553, 0.53652875, 0.69534344, 0.36302062,\n",
       "       0.39300833, 0.21280896, 0.14510447, 0.55135873, 0.70105148,\n",
       "       0.12590468, 0.40928581, 0.46565615, 0.47264782, 0.20546224,\n",
       "       0.26117711, 0.17673085, 0.20065547, 0.28835177, 0.23965588,\n",
       "       0.23842687, 0.23747098, 0.40046429, 0.2277755 , 0.18973098,\n",
       "       0.2684419 , 0.15654786, 0.30981838, 0.29862078, 0.24981565,\n",
       "       0.29094633, 0.27592517, 0.13024717, 0.20027311, 0.25440393,\n",
       "       0.21581319, 0.20284037, 0.25888297, 0.23023351, 0.22458009,\n",
       "       0.15157722, 0.26677591, 0.66677591, 0.67849242, 0.47106377,\n",
       "       0.24383449, 0.3705312 , 0.10146115, 0.13969685, 0.20079203,\n",
       "       0.1476171 , 0.14808139, 0.18110064, 0.08813328, 0.10411034])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 - 2s - loss: 0.0146 - val_loss: 0.0269\n",
      "Epoch 2/50\n",
      "154/154 - 0s - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 3/50\n",
      "154/154 - 0s - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 4/50\n",
      "154/154 - 0s - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 5/50\n",
      "154/154 - 0s - loss: 8.8435e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "154/154 - 0s - loss: 7.2948e-04 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "154/154 - 0s - loss: 5.8782e-04 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "154/154 - 0s - loss: 4.9040e-04 - val_loss: 0.0013\n",
      "Epoch 9/50\n",
      "154/154 - 0s - loss: 4.1302e-04 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "154/154 - 0s - loss: 3.5986e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "154/154 - 0s - loss: 3.2157e-04 - val_loss: 0.0011\n",
      "Epoch 12/50\n",
      "154/154 - 0s - loss: 2.8872e-04 - val_loss: 9.8744e-04\n",
      "Epoch 13/50\n",
      "154/154 - 0s - loss: 2.7002e-04 - val_loss: 9.3965e-04\n",
      "Epoch 14/50\n",
      "154/154 - 0s - loss: 2.5286e-04 - val_loss: 8.8125e-04\n",
      "Epoch 15/50\n",
      "154/154 - 0s - loss: 2.3644e-04 - val_loss: 8.1163e-04\n",
      "Epoch 16/50\n",
      "154/154 - 0s - loss: 2.2423e-04 - val_loss: 7.7210e-04\n",
      "Epoch 17/50\n",
      "154/154 - 0s - loss: 2.1072e-04 - val_loss: 7.3144e-04\n",
      "Epoch 18/50\n",
      "154/154 - 0s - loss: 1.9897e-04 - val_loss: 6.8380e-04\n",
      "Epoch 19/50\n",
      "154/154 - 0s - loss: 1.8951e-04 - val_loss: 6.5118e-04\n",
      "Epoch 20/50\n",
      "154/154 - 0s - loss: 1.8131e-04 - val_loss: 6.3398e-04\n",
      "Epoch 21/50\n",
      "154/154 - 0s - loss: 1.7157e-04 - val_loss: 6.4775e-04\n",
      "Epoch 22/50\n",
      "154/154 - 0s - loss: 1.6163e-04 - val_loss: 6.1582e-04\n",
      "Epoch 23/50\n",
      "154/154 - 0s - loss: 1.6150e-04 - val_loss: 6.2619e-04\n",
      "Epoch 24/50\n",
      "154/154 - 0s - loss: 1.5115e-04 - val_loss: 6.0345e-04\n",
      "Epoch 25/50\n",
      "154/154 - 0s - loss: 1.4442e-04 - val_loss: 6.1507e-04\n",
      "Epoch 26/50\n",
      "154/154 - 0s - loss: 1.3779e-04 - val_loss: 5.8251e-04\n",
      "Epoch 27/50\n",
      "154/154 - 0s - loss: 1.3044e-04 - val_loss: 5.5570e-04\n",
      "Epoch 28/50\n",
      "154/154 - 0s - loss: 1.2733e-04 - val_loss: 5.0912e-04\n",
      "Epoch 29/50\n",
      "154/154 - 0s - loss: 1.1937e-04 - val_loss: 4.7275e-04\n",
      "Epoch 30/50\n",
      "154/154 - 0s - loss: 1.1430e-04 - val_loss: 4.4947e-04\n",
      "Epoch 31/50\n",
      "154/154 - 0s - loss: 1.0847e-04 - val_loss: 4.0075e-04\n",
      "Epoch 32/50\n",
      "154/154 - 0s - loss: 1.0672e-04 - val_loss: 3.9278e-04\n",
      "Epoch 33/50\n",
      "154/154 - 0s - loss: 1.0194e-04 - val_loss: 3.3398e-04\n",
      "Epoch 34/50\n",
      "154/154 - 0s - loss: 9.6749e-05 - val_loss: 3.2689e-04\n",
      "Epoch 35/50\n",
      "154/154 - 0s - loss: 9.2191e-05 - val_loss: 2.9616e-04\n",
      "Epoch 36/50\n",
      "154/154 - 0s - loss: 9.6047e-05 - val_loss: 2.7258e-04\n",
      "Epoch 37/50\n",
      "154/154 - 0s - loss: 9.2692e-05 - val_loss: 2.5541e-04\n",
      "Epoch 38/50\n",
      "154/154 - 0s - loss: 8.8172e-05 - val_loss: 2.4626e-04\n",
      "Epoch 39/50\n",
      "154/154 - 0s - loss: 7.9681e-05 - val_loss: 2.0652e-04\n",
      "Epoch 40/50\n",
      "154/154 - 0s - loss: 9.1759e-05 - val_loss: 2.1144e-04\n",
      "Epoch 41/50\n",
      "154/154 - 0s - loss: 7.1392e-05 - val_loss: 1.9031e-04\n",
      "Epoch 42/50\n",
      "154/154 - 0s - loss: 7.4510e-05 - val_loss: 1.7476e-04\n",
      "Epoch 43/50\n",
      "154/154 - 0s - loss: 8.2113e-05 - val_loss: 1.7423e-04\n",
      "Epoch 44/50\n",
      "154/154 - 0s - loss: 7.4799e-05 - val_loss: 1.5832e-04\n",
      "Epoch 45/50\n",
      "154/154 - 0s - loss: 7.7278e-05 - val_loss: 1.5748e-04\n",
      "Epoch 46/50\n",
      "154/154 - 0s - loss: 6.8028e-05 - val_loss: 1.5063e-04\n",
      "Epoch 47/50\n",
      "154/154 - 0s - loss: 6.8866e-05 - val_loss: 1.4615e-04\n",
      "Epoch 48/50\n",
      "154/154 - 0s - loss: 6.1792e-05 - val_loss: 1.4276e-04\n",
      "Epoch 49/50\n",
      "154/154 - 0s - loss: 5.2822e-05 - val_loss: 1.4259e-04\n",
      "Epoch 50/50\n",
      "154/154 - 0s - loss: 5.1595e-05 - val_loss: 1.3474e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5RdZZ3m8e/v3OpeFVJJhVywE5tok0QBCUgPMq0wYKJI6AY0Ki2zhlmxe8kMTje20L2gW5ZOy5o16jigDgg22o1Ao7RpCYI0ME53K1CB2CRETIFoKkWSyrUuqcu5/OaPvU/VqUMVdZKqyiH1Pp+1ztqX8+593k0O9Zz3fffF3B0REQlPotoVEBGR6lAAiIgESgEgIhIoBYCISKAUACIigUpVuwJHY968eb506dJqV0NE5ISyefPmfe4+v3z9CRUAS5cupb29vdrVEBE5oZjZr8dbry4gEZFAKQBERAKlABARCdQJNQYgInK0stksnZ2dDA4OVrsqM662tpYlS5aQTqcrKq8AEJFZrbOzk6amJpYuXYqZVbs6M8bd2b9/P52dnSxbtqyibdQFJCKz2uDgIK2trbP6jz+AmdHa2npULR0FgIjMerP9j3/R0R5nGAHw9B2w9XvVroWIyJtKGAGw+Vuw9fvVroWIBOjQoUN87WtfO+rtPvCBD3Do0KEZqNGoMAIg0wDZI9WuhYgEaKIAyOfzb7jdpk2bmDNnzkxVCwjlLKBMAwz3V7sWIhKgG264gZdffpkzzjiDdDpNY2MjCxcuZMuWLbz44otcdtll7Ny5k8HBQa677jo2bNgAjN76pq+vj7Vr1/Ke97yHf/3Xf2Xx4sX84Ac/oK6ubsp1CyMA0g3Qv7/atRCRKvvcP27jxa6ead3nikXN/OWHVk74/he/+EW2bt3Kli1beOqpp/jgBz/I1q1bR07VvPvuu5k7dy4DAwOcffbZXH755bS2to7Zx44dO/jud7/LnXfeyYc//GG+973vcdVVV0257mEEQKYBhvuqXQsREc4555wx5+l/9atf5aGHHgJg586d7Nix43UBsGzZMs444wwAzjrrLF599dVpqUsgAVCvMQARecNf6sdLQ0PDyPxTTz3F448/zk9/+lPq6+t573vfO+55/DU1NSPzyWSSgYGBaalLIIPAjRoDEJGqaGpqore3d9z3Dh8+zEknnUR9fT2/+MUv+NnPfnZc6xZGCyBdHwWAOwRyQYiIvDm0trZy3nnnsWrVKurq6liwYMHIe2vWrOEb3/gG73znO3n729/Oueeee1zrFkYAZBoAh+xA1B0kInIc3XvvveOur6mp4ZFHHhn3vWI//7x589i6devI+uuvv37a6hVIF1Dc56ZuIBGREWEFQFYBICJSVFEAmNkaM3vJzDrM7IZx3q8xs/vj9582s6Xx+ovMbLOZvRBPLyjZ5ql4n1viV9t0HdTrpONuH7UARERGTDoGYGZJ4HbgIqATeNbMNrr7iyXFrgEOuvupZrYeuBX4CLAP+JC7d5nZKuBRYHHJdh9395l/ynumMZoO61RQEZGiSloA5wAd7v6Kuw8D9wHrysqsA+6J5x8ELjQzc/fn3b0rXr8NqDWzGo634sCvLgYTERlRSQAsBnaWLHcy9lf8mDLungMOA61lZS4Hnnf3oZJ134q7f26yCW5kbWYbzKzdzNq7u7srqO44RsYA1AIQESmqJADG+8PsR1PGzFYSdQt9suT9j7v7O4Dz49cfjvfh7n6Hu69299Xz58+voLrjSOssIBGpjmO9HTTAV77yFY4cmbkfrpUEQCdwSsnyEqBrojJmlgJagAPx8hLgIeAT7v5ycQN33xVPe4F7ibqaZoZOAxWRKnkzB0AlF4I9Cyw3s2XALmA98LGyMhuBq4GfAlcAT7i7m9kc4GHgRnf/l2LhOCTmuPs+M0sDlwCPT/loJqIAEJEqKb0d9EUXXURbWxsPPPAAQ0ND/P7v/z6f+9zn6O/v58Mf/jCdnZ3k83luuukm9uzZQ1dXF+973/uYN28eTz755LTXbdIAcPecmV1LdAZPErjb3beZ2S1Au7tvBO4CvmNmHUS//NfHm18LnArcZGY3xesuBvqBR+M//kmiP/53TuNxjaUxABEBeOQG2P3C9O7z5HfA2i9O+Hbp7aAfe+wxHnzwQZ555hncnUsvvZSf/OQndHd3s2jRIh5++GEgukdQS0sLX/rSl3jyySeZN2/e9NY5VtGtINx9E7CpbN3NJfODwJXjbPd54PMT7Pasyqs5RYkkpGp1FpCIVNVjjz3GY489xplnnglAX18fO3bs4Pzzz+f666/ns5/9LJdccgnnn3/+calPGPcCgviGcGoBiATtDX6pHw/uzo033sgnP/nJ1723efNmNm3axI033sjFF1/MzTffPM4eplcYt4IA3RJaRKqi9HbQ73//+7n77rvp64t6I3bt2sXevXvp6uqivr6eq666iuuvv57nnnvuddvOhHBaAJl6dQGJyHFXejvotWvX8rGPfYzf/d3fBaCxsZG//du/paOjg8985jMkEgnS6TRf//rXAdiwYQNr165l4cKFMzIIbO7lp/S/ea1evdrb24/xzhF3XgB1J8FV35veSonIm9r27ds57bTTql2N42a84zWzze6+urxsOF1AxYfCiIgIEFIAaAxARGSMgAJALQCRUJ1IXd1TcbTHGVAANOhCMJEA1dbWsn///lkfAu7O/v37qa2trXibcM4CSjeoBSASoCVLltDZ2ckx3034BFJbW8uSJUsqLh9OAGTiAHCH8e88LSKzUDqdZtmyZdWuxptSQF1A9eB5yA1NXlZEJAABBUD8WEiNA4iIACEFQFqPhRQRKRVOAIw8E0AtABERCCoA4i4gnQkkIgIEFQBxF1BWASAiAkEFgB4LKSJSKpwASCsARERKhRMAagGIiIwRUAAUTwNVAIiIQEgBUOwC0iCwiAgQUgCkMpBIqwUgIhILJwAgviGcLgQTEYEgA0AtABERCDEANAYgIgKEFgB6MLyIyIiwAiDTqDEAEZFYYAFQr9tBi4jEKgoAM1tjZi+ZWYeZ3TDO+zVmdn/8/tNmtjRef5GZbTazF+LpBSXbnBWv7zCzr5odh+c06sHwIiIjJg0AM0sCtwNrgRXAR81sRVmxa4CD7n4q8GXg1nj9PuBD7v4O4GrgOyXbfB3YACyPX2umcByV0YPhRURGVNICOAfocPdX3H0YuA9YV1ZmHXBPPP8gcKGZmbs/7+5d8fptQG3cWlgINLv7T93dgW8Dl035aCaTaVAXkIhIrJIAWAzsLFnujNeNW8bdc8BhoLWszOXA8+4+FJfvnGSfAJjZBjNrN7P27u7uCqr7BnQhmIjIiEoCYLy+eT+aMma2kqhb6JNHsc9opfsd7r7a3VfPnz+/guq+gUw9FLKQG57afkREZoFKAqATOKVkeQnQNVEZM0sBLcCBeHkJ8BDwCXd/uaT8kkn2Of2Kj4XUxWAiIhUFwLPAcjNbZmYZYD2wsazMRqJBXoArgCfc3c1sDvAwcKO7/0uxsLu/BvSa2bnx2T+fAH4wxWOZXFq3hBYRKZo0AOI+/WuBR4HtwAPuvs3MbjGzS+NidwGtZtYB/AlQPFX0WuBU4CYz2xK/2uL3/hj4JtABvAw8Ml0HNaGRh8JoHEBEJFVJIXffBGwqW3dzyfwgcOU4230e+PwE+2wHVh1NZadsJAB0JpCISGBXAhcfCqMWgIhIWAGgB8OLiIwIKwD0YHgRkRGBBYDOAhIRKQosAIrXAWgMQEQkrAAYuQ5AZwGJiIQVAKkasKSuAxARIbQAMNOD4UVEYmEFAOiW0CIisTADQIPAIiIBBkC6Xl1AIiKEGACZRgWAiAhBBoBaACIiEGQAaAxARARCDIC0TgMVEYEQA0DXAYiIAEEGgMYAREQgyABohPwQ5HPVromISFWFFwDFG8Jl1QoQkbCFFwB6MLyICBB0AKgFICJhCzgAdEM4EQlbeAEwMgagLiARCVt4AVB8LKS6gEQkcAEGgB4MLyICQQaABoFFRCDIAIi7gDQGICKBCy8AioPAOgtIRAJXUQCY2Roze8nMOszshnHerzGz++P3nzazpfH6VjN70sz6zOy2sm2eive5JX61TccBTSpdB5guBBOR4KUmK2BmSeB24CKgE3jWzDa6+4slxa4BDrr7qWa2HrgV+AgwCNwErIpf5T7u7u1TPIajY6Y7goqIUFkL4Bygw91fcfdh4D5gXVmZdcA98fyDwIVmZu7e7+7/TBQEbx6ZBt0LSESCV0kALAZ2lix3xuvGLePuOeAw0FrBvr8Vd//cZGY2XgEz22Bm7WbW3t3dXcEuK6AHw4uIVBQA4/1h9mMoU+7j7v4O4Pz49YfjFXL3O9x9tbuvnj9//qSVrUimUWMAIhK8SgKgEzilZHkJ0DVRGTNLAS3AgTfaqbvviqe9wL1EXU0z4i8eeoHbn+wYXZGp11lAIhK8SgLgWWC5mS0zswywHthYVmYjcHU8fwXwhLtP2AIws5SZzYvn08AlwNajrXylXth1mKd/VZJHejC8iMjkZwG5e87MrgUeBZLA3e6+zcxuAdrdfSNwF/AdM+sg+uW/vri9mb0KNAMZM7sMuBj4NfBo/Mc/CTwO3DmtR1airamWzoMlf/DT9dC7e6Y+TkTkhDBpAAC4+yZgU9m6m0vmB4ErJ9h26QS7PauyKk7dguYanvvNwdEVmUZ1AYlI8IK4EnhBcy0H+ocZyuWjFZl6DQKLSPACCYAaALp7h6IVuhBMRCSMAGhrqgVgbzEA0g2QG4BCvoq1EhGprjACIG4B7O2JL0gu3hJaZwKJSMCCCIAFzVELYE9PsQuoeEdQBYCIhCuIAJhbnyGVMPaMtACKj4XUmUAiEq4gAiCRMNqaakpaAOoCEhEJIgAA5jfXsrc3bgGk9VxgEZFgAmBBUw17R1oAxS4gBYCIhCucAGiuZU+xBZBRC0BEJKAAqOHQkSyD2bzGAERECCgA2uJTQbt7h6ILwUBnAYlI0MIJgKb4YrDewdEWgLqARCRgwQTAmIvB0roQTEQkwAAYhEQifi6wuoBEJFzBBMBJ9WnSSRu9GCxdr0FgEQlaMAFgZrQ11Y69IZzGAEQkYMEEAER3Bd2rZwKIiACBBcCCptqSG8IpAEQkbGEFQHPNaABoDEBEAhdUALQ119IzmIuvBm5UC0BEghZWABQvBusZUheQiAQvqAAYuRagdzC6IZwCQEQCFmYA9AxGXUAaAxCRgAUWAFEX0MjtIIb7wb3KtRIRqY6gAqClLk0mlYguBss0AA7ZgWpXS0SkKoIKgOhq4PhiMN0RVEQCF1QAQPxksJ7SW0LrhnAiEqaKAsDM1pjZS2bWYWY3jPN+jZndH7//tJktjde3mtmTZtZnZreVbXOWmb0Qb/NVM7PpOKDJjFwMVrwltAaCRSRQkwaAmSWB24G1wArgo2a2oqzYNcBBdz8V+DJwa7x+ELgJuH6cXX8d2AAsj19rjuUAjlZ0Q7ghPRheRIJXSQvgHKDD3V9x92HgPmBdWZl1wD3x/IPAhWZm7t7v7v9MFAQjzGwh0OzuP3V3B74NXDaVA6nUguZaeodyDFp0RpACQERCVUkALAZ2lix3xuvGLePuOeAw0DrJPjsn2ScAZrbBzNrNrL27u7uC6r6x4tXAB7LpaIUCQEQCVUkAjNc3X37yfCVljqm8u9/h7qvdffX8+fPfYJeVKV4M1j2UjFZoDEBEAlVJAHQCp5QsLwG6JipjZimgBTgwyT6XTLLPGVG8GGz3YLEFoLOARCRMlQTAs8ByM1tmZhlgPbCxrMxG4Op4/grgibhvf1zu/hrQa2bnxmf/fAL4wVHX/hi0xS2A3QNxI0QPhheRQKUmK+DuOTO7FngUSAJ3u/s2M7sFaHf3jcBdwHfMrIPol//64vZm9irQDGTM7DLgYnd/Efhj4G+AOuCR+DXjmmtT1KYT7OovBoDGAEQkTJMGAIC7bwI2la27uWR+ELhygm2XTrC+HVhVaUWnS/HZwHv68pCsgawCQETCFNyVwFByMZieCSAiAQsyANqaSy4G0xiAiAQqyAAYeTh8pl5nAYlIsIIMgLbmGvqH8+RTeiqYiIQryAAoXgswnKjVhWAiEqwwA6ApuhZgwOrUBSQiwQoyAIoXgx3xGnUBiUiwggyAYhfQgcRc6OmCfLbKNRIROf6CDIDGmhR16SQvp5ZDbhC6f1HtKomIHHdBBoCZsaC5hn/jt6MVu56rboVERKogyACAaBxg20Ar1LZAlwJARMITbAAsaK5lb+8QLDoTup6vdnVERI67cAOgqYY9PUP4wjNhzzbIDk6+kYjILBJsALQ11zCQzTPYdjoUclEIiIgEJNgAKD4ack/jimiFxgFEJDDBBkBbfDVwV2EuNMzXmUAiEpxgA6B4MdieviFY9C4NBItIcIINgOLtIPb0xGcC7XsJhnRfIBEJR7AB0FiToiGTjB4Ms/hd4AXY/W/VrpaIyHETbABANBC8p3cwagGAxgFEJChBB0Bbcw17ewahsQ2al+hMIBEJStABsKC5lq5D8QVgi87QQLCIBCXoAFje1siuQwP0DGajcYADr8DAwWpXS0TkuAg6AFYubgFge1dPdCooQNeWKtZIROT4CTsAFjUDsK2rJ+oCAo0DiEgwgg6AtqZa2ppq2Np1GOpOgrlv1TiAiAQj6ACAqBXwYldPtLDoTNilABCRMFQUAGa2xsxeMrMOM7thnPdrzOz++P2nzWxpyXs3xutfMrP3l6x/1cxeMLMtZtY+HQdzLFYtbmHH3j4Gs/loHKCnE/r2Vqs6IiLHzaQBYGZJ4HZgLbAC+KiZrSgrdg1w0N1PBb4M3BpvuwJYD6wE1gBfi/dX9D53P8PdV0/5SI7RykXN5AvOS7t7Ry8IUzeQiASgkhbAOUCHu7/i7sPAfcC6sjLrgHvi+QeBC83M4vX3ufuQu/8K6Ij396axclF0JtDWrsOw8HSwhK4IFpEgVBIAi4GdJcud8bpxy7h7DjgMtE6yrQOPmdlmM9sw0Yeb2QYzazez9u7u7gqqe3SWnFRHS106OhOophHmvV0tABEJQiUBYOOs8wrLvNG257n7u4i6lj5lZv9+vA939zvcfbW7r54/f34F1T06ZsbKRc1s23U4WrHozOhUUC8/RBGR2aWSAOgETilZXgJ0TVTGzFJAC3DgjbZ19+J0L/AQVewaWrmome27e8nmC9EVwf3dcLizWtURETkuKgmAZ4HlZrbMzDJEg7oby8psBK6O568AnnB3j9evj88SWgYsB54xswYzawIwswbgYmDr1A/n2Kxc1MJwrsDL3X0aCBaRYEwaAHGf/rXAo8B24AF332Zmt5jZpXGxu4BWM+sA/gS4Id52G/AA8CLwI+BT7p4HFgD/bGY/B54BHnb3H03voVVu1eL4iuBdPbBgFSRSuiJYRGa9VCWF3H0TsKls3c0l84PAlRNs+wXgC2XrXgFOP9rKzpRl8xqpSyfZ1tXD5WctgbYVagGIyKwX/JXAAMmEcdrCpuhUUIjGAbqe10CwiMxqCoDYykUtbO/qoVBw+K33wOBheOHvq10tEZEZowCIrVrcTO9Qjt8cOAKr/gCWnA2PfBb6pv/aAxGRNwMFQKx4RfC2rh5IJOHS22C4Dx75TJVrJiIyMxQAseULGkknbXQcoO134Pf+DLY9BNv/sbqVExGZAQqAWE0qyfK2pqgFUHTep+Hkd8DDf6pHRYrIrKMAKLFqcXRLCC+e/ZNMw7rboX8fPPoX1a2ciMg0UwCUWLmohf39w+zpGRpdufB0eM+nYcvfQcfj1auciMg0UwCUKF4RvLV4Y7iif/9n0V1C//HTMNRbhZqJiEw/BUCJ3zm5GTPGjgMApGth3W3RDeIe/6uq1E1EZLopAEo01KRYNq+BbV2HX//mKefAuX8Mz34TnvoiZAeOfwVFRKaRAqDMqkUtr28BFF1wE6z8A3jqr+G2s6NTRHW7CBE5QSkAyqxc1MyuQwMc7B9+/ZuZerjyW3D1D6G2Bf7+P8I9H4LdVbuTtYjIMVMAlFm1uOSK4IksOx82/F/44P+EPVvh/5wPP/wT3TZCRE4oCoAyKxfFzwYYbxygVDIFZ/9n+C/PRdPNfwNfWQUPXw8Hfz3zFRURmSIFQJk59RkWz6lj6xu1AErVz4UP/A/41NPwjiuiIPjqmfD9DbBn24zWVURkKhQA41i5qHnyFkC5ecujq4av2wLv/iPY/kP4+r+Dez8CrzwFhcKM1FVE5FgpAMaxanELv9rXz+ZfHzj6jVuWwJr/Dv9tK7z3z2HnM/DtdVH30I//EvZun/4Ki4gcA/MT6DTG1atXe3t7+4x/TtehAT5658/oOjTAX126ko+d8xbM7Nh2lh2AlzbBz++PbiXh+egGc+9cD6suh+aF01t5EZEyZrbZ3Ve/br0CYHyHj2S57v7neeqlbj6y+hQ+t24ltenk1Hba1w3bvg8/v2/0ofMLT4flF0evxWdFzyIQEZlGCoBjkC84X3n8l/zvJzo4/ZQ5fOOqd7GwpW56dr5vB2zfCDt+DDufBi9A3Unw2xfCqRdGD6ZvPRVqGqfn80QkWAqAKfjR1t386QNbqMskuf1j7+Ldb22d3g8YOAgvPxGFQcfj0F9yPUHz4igI5r0tGmhuOQVaFkfr61vhWLumRCQYCoAp6tjby4Zvb+bXB47we2+bz5qVJ3PRigWc1JCZ3g8qFGDfS7Dvl1ErYd8O2B9Ph8pOTU3WQPOiaOC5sS0KhJHX3LLlVkjVTG9dReSEoACYBj2DWW57ooOH/+01dh0aIJkwzn3rXNasWsj7Vyygrbl25j7cPWoZHN4JPV1weBf0dI7O93fDkf0weGjifWSaSoJhLtQ0Q00T1DbH8/Fy3UllrzkKD5ETmAJgGrk7W3f18KNtr/HI1t280t2PGSyb18BpC5tZsbCZ0xY2cdrCZk5urj32M4iORT4XdSkd2Q9H9sGRA/F0fzy/P3rC2cCB6NkGgz3RNDfJ3U3T9VFwNMyD+nnxNF6ubYFMI2Qa4lf5fCOkprmlJCIVUwDMEHenY28fj724h5/vPMT23T3sPDD6x3ROfZq3L2jibQuaeNuCRpbH83Onu+toqnLDMNwXtSAGDsXTg/Hr0Gio9O+LAqV/f9TqmCw4ihLpaEC7GAg1TSWtj6bR1ke6HtJ1r58mM9EjOhOpaJrMRPMAhTwUcvErGy1j0XMcivtI1UbTZHrsNp6P5r0Q77cmus2HyCwyUQDomz5FZsbyBU0sX9A0sq53MMsvdvey/bUetr/Ww0u7e/mH53fRO5QbKTOvMcNb5zeyeE4di+bUsrCljsVz6lgYzzfXpo5vyyGVgdTcqGvoaAz3Ry2I4f4oQIb7R+eH+srWx9OhuNUxeAgO/SaaH+qB7JGZObajZYkoCFKZaApjw6IYHonk2JBK1cXLtdF8qiYKnlRN/H7taPAVA684n6mP9pNpGN2fBvhlhikAZkBTbZqzl87l7KWjf0zdnd09g/xyTx879vTyyz29/GpfP8/86gB7egbJFca2xDKpBPMba5jXVMP8xgzzm2qY11hDS12a5ro0zbVpmutSNNemaalL01iToi6TpCaVOL7BUezqmQ75XNSiyA5EYVA6zQ9H7xeykI9fhSxgUUsgkRxtHSRS0S/67ADkBkv2NQC5oaisJSGRiKfJ6I9+Pht9Tm4I8kNRqyg/FH9GvP9i+UQyCoLsQEmdS+o72BPtJzc4Oi2WrYjFQVA7NoyK03Rd1PVW01wyhtMU/VsUj88S0atY39qWaEyndk48bdF1J4GrKADMbA3wv4Ak8E13/2LZ+zXAt4GzgP3AR9z91fi9G4FrgDzwX9390Ur2OduYGQtb6ljYUsfvvW3+mPfyBae7d4hdhwZ47fAArx0aZF/fEN29Q3T3DdF5cIAtOw9zoH+IwiQ9dsmEUZ9JUp9J0pCJQqEunaQuk6Q2Hc+nk9SmE6SSCdLJBJmkjcynk0YqYaRTCdKJBKnie4lomkrayPqobDSfTETbJRMJkmYkk8XlaJpKJsYsjxtSyRQk41/Fs1U+N9oCGnn1RC2j7BEYPgLZ/mg63B8FR2kYFafDR+DAK6NjOEM9wNF251oUHslMHBYloWE22oJJ15W0cIqtm7JAStWUdNOl43/LzOj8SNCWBG4iGe0r01DSPRi3gMq/H+5RqGNRcMu0mDQAzCwJ3A5cBHQCz5rZRnd/saTYNcBBdz/VzNYDtwIfMbMVwHpgJbAIeNzM3hZvM9k+g5FMGCe31HJySy1w0oTl8gWnbyhHz0CWnsEsPQM5egazHB7IcmQoR/9wniPDOY4M5zkylKd/OMfAcJ7BXJ7+oRz7+oYZzOYZGM4zkM2TyxfI5p3h/PG/UZ0ZI4GQtHgavxI2Ok0kIGlGIi6XMMOMkveNpFEyH28brx+zv/j9iRpIiXj/qWLZks+O3iP+/NH5hBEvx+sS0f4TNvpZY96LD94AsyaMZswWj+4vZSQzhjWO1if6jOhzou1K6xIvUyCdO0IyN4CRx3CsUMCsgHmBRCFLOtdHaugQqeHD0WvoEOnhw1ghh1EAd8wLGAUSnscKWRK5QSw/SOJIL5brHlkmP4zlh7H8UDT16fsOebHFFf/Rt7Jg80QcLsVXKoMVx4SKgZNI4YmoNehm4KPx6MWFRBISaSwZbRNN05glojqU1if61mKJBGYJLJEoCcv4hY0GVzFER9YV/xHLyo2nGMQjnxEvv/uPpn18qpK9nQN0uPsrAGZ2H7AOKP1jvQ74q3j+QeA2i37irQPuc/ch4Fdm1hHvjwr2KWWSCaOlLurymU7uTr7g5ApRGOTyTjZfIBvP5wpRUOTyTrYQr8sXyBaiaa4QbV98Rcuj63P5seuzeafg0XJhZH388mhdcd6dknmnUGBkPloPhUK0v+gzCgzlRtfnS94r7nvc/wZAobj/uE7F7QpxPQru8Wv0M50T7amg9fFreu9BlSRPhixp8qTiV8ZypMiRIk+SAkkKJEamTpI8tZalgUHqGaTBRqcp8nEpcBIU3HCMhBWifZMjTY4astHUciTjz02SJ02WFIOkLD9ufQ0nSbSvqM65kXonzMeUK4piwUlENXaJAiwAAAVZSURBVIpfPmZ9MS6sZH1xH2Yef+6xfWGG3vWfqKlCACwGdpYsdwLvnqiMu+fM7DDQGq//Wdm2i+P5yfYJgJltADYAvOUtb6mgunK0zCzu6mHq9zsKkI8JiLKgcMfj0CoGSfEXaDE8nLGhMrKPQjTvRMHkxNsX90s8Hfl8Rj4DSn7pMvpZpdtRUn50X9HxFD+3uA2l23v845TR1lSxdeLxfw9Kjq28zq+vbzFoGQndqJWXGGmNFVuLOGQLBbK56IfEkfiHSjZfGKnP2FYWIy3AMa2xuOLR543Wo1iH0n0VW1kelx37AyT6dyrt0iz9be/EP0RKviPFYy3fR/QdKLZ4CiQ8br0Rtcz+Ojn91+JUEgDjtVXKI2yiMhOtH68Tb9xYdPc7gDsgOg104mqKVIcVu3rG/bqLvHlVMprSCZxSsrwE6JqojJmlgBbgwBtsW8k+RURkBlUSAM8Cy81smZlliAZ1N5aV2QhcHc9fATzhUTtwI7DezGrMbBmwHHimwn2KiMgMmrQLKO7TvxZ4lOiUzbvdfZuZ3QK0u/tG4C7gO/Eg7wGiP+jE5R4gGtzNAZ9y9zzAePuc/sMTEZGJ6FYQIiKz3ES3gtAVFSIigVIAiIgESgEgIhIoBYCISKBOqEFgM+sGfn2Mm88D9k1jdU4UOu6w6LjDUulx/5a7zy9feUIFwFSYWft4o+CznY47LDrusEz1uNUFJCISKAWAiEigQgqAO6pdgSrRcYdFxx2WKR13MGMAIiIyVkgtABERKaEAEBEJ1KwPADNbY2YvmVmHmd1Q7frMJDO728z2mtnWknVzzezHZrYjnk780OETlJmdYmZPmtl2M9tmZtfF62f1sZtZrZk9Y2Y/j4/7c/H6ZWb2dHzc98e3XJ91zCxpZs+b2Q/j5Vl/3Gb2qpm9YGZbzKw9XnfM3/NZHQAlD7RfC6wAPho/qH62+htgTdm6G4B/cvflwD/Fy7NNDvhTdz8NOBf4VPzvPNuPfQi4wN1PB84A1pjZucCtwJfj4z4IXFPFOs6k64DtJcuhHPf73P2MkvP/j/l7PqsDgJIH2rv7MFB8+Pys5O4/IXoeQ6l1wD3x/D3AZce1UseBu7/m7s/F871EfxQWM8uP3SN98WI6fjlwAfBgvH7WHTeAmS0BPgh8M142AjjuCRzz93y2B8B4D7RfPEHZ2WqBu78G0R9KoK3K9ZlRZrYUOBN4mgCOPe4G2QLsBX4MvAwccvdcXGS2fue/AvwZUIiXWwnjuB14zMw2m9mGeN0xf88reSj8iaySB9rLLGFmjcD3gE+7e0/0o3B2i5+wd4aZzQEeAk4br9jxrdXMMrNLgL3uvtnM3ltcPU7RWXXcsfPcvcvM2oAfm9kvprKz2d4C0MPnYY+ZLQSIp3urXJ8ZYWZpoj/+f+fu349XB3HsAO5+CHiKaAxkjpkVf9zNxu/8ecClZvYqUbfuBUQtgtl+3Lh7VzzdSxT45zCF7/lsDwA9fD463qvj+auBH1SxLjMi7v+9C9ju7l8qeWtWH7uZzY9/+WNmdcB/IBr/eBK4Ii42647b3W909yXuvpTo/+kn3P3jzPLjNrMGM2sqzgMXA1uZwvd81l8JbGYfIPp1UHz4/BeqXKUZY2bfBd5LdIvYPcBfAv8APAC8BfgNcKW7lw8Un9DM7D3A/wNeYLRP+M+JxgFm7bGb2TuJBv2SRD/mHnD3W8zsrUS/jOcCzwNXuftQ9Wo6c+IuoOvd/ZLZftzx8T0UL6aAe939C2bWyjF+z2d9AIiIyPhmexeQiIhMQAEgIhIoBYCISKAUACIigVIAiIgESgEgIhIoBYCISKD+P/tptSvg5KewAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(13, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu'))\n",
    "model.add(Dense(38, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 7.436\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -12:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -12:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.383717526429941"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.345780418376556"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13-38-1 Test RMSE: 62.006\n",
    "\n",
    "13-68-1 Test RMSE: 68.752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 61-1 Test RMSE: Test RMSE: 63.163\n",
    "\n",
    "13-40-1 Test RMSE: 67.366\n",
    "\n",
    "13-100-1 Test RMSE: 65.965\n",
    "\n",
    "13- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
