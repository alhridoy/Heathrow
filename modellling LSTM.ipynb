{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score , classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first load the data and take a look at what we have.\n",
    "df = pd.read_csv('Heathrow_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/7/18</td>\n",
       "      <td>232.1240</td>\n",
       "      <td>59.746988</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/7/18</td>\n",
       "      <td>168.0445</td>\n",
       "      <td>73.870523</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/7/18</td>\n",
       "      <td>194.0030</td>\n",
       "      <td>59.394005</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13/7/18</td>\n",
       "      <td>343.2730</td>\n",
       "      <td>68.192323</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14/7/18</td>\n",
       "      <td>190.1570</td>\n",
       "      <td>78.645600</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>6/1/21</td>\n",
       "      <td>85.2440</td>\n",
       "      <td>40.983786</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>7/1/21</td>\n",
       "      <td>163.9400</td>\n",
       "      <td>37.200143</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>9/1/21</td>\n",
       "      <td>282.0585</td>\n",
       "      <td>58.818259</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>16/1/21</td>\n",
       "      <td>147.2020</td>\n",
       "      <td>37.496792</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>214.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>17/1/21</td>\n",
       "      <td>74.6270</td>\n",
       "      <td>37.605938</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>269.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp   wdir  wspd  \\\n",
       "0    10/7/18   232.1240   59.746988  18.7  15.5  22.6   0.0   75.0  12.5   \n",
       "1    11/7/18   168.0445   73.870523  18.6  13.9  23.4   0.0   45.0  10.1   \n",
       "2    12/7/18   194.0030   59.394005  19.2  13.7  24.4   0.0   52.0   8.4   \n",
       "3    13/7/18   343.2730   68.192323  20.6  15.7  26.8   0.0  135.0  10.1   \n",
       "4    14/7/18   190.1570   78.645600  21.8  14.9  27.8   0.0  177.0  10.2   \n",
       "..       ...        ...         ...   ...   ...   ...   ...    ...   ...   \n",
       "772   6/1/21    85.2440   40.983786   3.2   0.1   5.6   0.5   89.0  11.3   \n",
       "773   7/1/21   163.9400   37.200143  -0.1  -1.7   2.1   0.0  277.0  10.6   \n",
       "774   9/1/21   282.0585   58.818259   0.1  -2.1   2.6   0.0  178.0   6.6   \n",
       "775  16/1/21   147.2020   37.496792   4.8  -0.8   8.3   0.5  214.0  14.8   \n",
       "776  17/1/21    74.6270   37.605938   5.3   2.2   8.1   5.3  269.0  12.8   \n",
       "\n",
       "       pres  \n",
       "0    1023.9  \n",
       "1    1021.8  \n",
       "2    1021.5  \n",
       "3    1021.8  \n",
       "4    1020.0  \n",
       "..      ...  \n",
       "772  1019.5  \n",
       "773  1018.6  \n",
       "774  1026.4  \n",
       "775  1020.0  \n",
       "776  1023.6  \n",
       "\n",
       "[581 rows x 10 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], \n",
    "               axis=1,\n",
    "              inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Nox_tropo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Nox_tropo'], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.1240</td>\n",
       "      <td>59.746988</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.0445</td>\n",
       "      <td>73.870523</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.0030</td>\n",
       "      <td>59.394005</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.2730</td>\n",
       "      <td>68.192323</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.1570</td>\n",
       "      <td>78.645600</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>85.2440</td>\n",
       "      <td>40.983786</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>163.9400</td>\n",
       "      <td>37.200143</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>282.0585</td>\n",
       "      <td>58.818259</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>147.2020</td>\n",
       "      <td>37.496792</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>214.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>74.6270</td>\n",
       "      <td>37.605938</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>269.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp   wdir  wspd    pres\n",
       "0     232.1240   59.746988  18.7  15.5  22.6   0.0   75.0  12.5  1023.9\n",
       "1     168.0445   73.870523  18.6  13.9  23.4   0.0   45.0  10.1  1021.8\n",
       "2     194.0030   59.394005  19.2  13.7  24.4   0.0   52.0   8.4  1021.5\n",
       "3     343.2730   68.192323  20.6  15.7  26.8   0.0  135.0  10.1  1021.8\n",
       "4     190.1570   78.645600  21.8  14.9  27.8   0.0  177.0  10.2  1020.0\n",
       "..         ...         ...   ...   ...   ...   ...    ...   ...     ...\n",
       "772    85.2440   40.983786   3.2   0.1   5.6   0.5   89.0  11.3  1019.5\n",
       "773   163.9400   37.200143  -0.1  -1.7   2.1   0.0  277.0  10.6  1018.6\n",
       "774   282.0585   58.818259   0.1  -2.1   2.6   0.0  178.0   6.6  1026.4\n",
       "775   147.2020   37.496792   4.8  -0.8   8.3   0.5  214.0  14.8  1020.0\n",
       "776    74.6270   37.605938   5.3   2.2   8.1   5.3  269.0  12.8  1023.6\n",
       "\n",
       "[581 rows x 9 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.32124000e+02, 5.97469878e+01, 1.87000000e+01, ...,\n",
       "        7.50000000e+01, 1.25000000e+01, 1.02390000e+03],\n",
       "       [1.68044500e+02, 7.38705230e+01, 1.86000000e+01, ...,\n",
       "        4.50000000e+01, 1.01000000e+01, 1.02180000e+03],\n",
       "       [1.94003000e+02, 5.93940048e+01, 1.92000000e+01, ...,\n",
       "        5.20000000e+01, 8.40000000e+00, 1.02150000e+03],\n",
       "       ...,\n",
       "       [2.82058500e+02, 5.88182587e+01, 1.00000000e-01, ...,\n",
       "        1.78000000e+02, 6.60000000e+00, 1.02640000e+03],\n",
       "       [1.47202000e+02, 3.74967921e+01, 4.80000000e+00, ...,\n",
       "        2.14000000e+02, 1.48000000e+01, 1.02000000e+03],\n",
       "       [7.46270000e+01, 3.76059383e+01, 5.30000000e+00, ...,\n",
       "        2.69000000e+02, 1.28000000e+01, 1.02360000e+03]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.1240</td>\n",
       "      <td>59.746988</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.0445</td>\n",
       "      <td>73.870523</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.0030</td>\n",
       "      <td>59.394005</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.2730</td>\n",
       "      <td>68.192323</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.1570</td>\n",
       "      <td>78.645600</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>85.2440</td>\n",
       "      <td>40.983786</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>163.9400</td>\n",
       "      <td>37.200143</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>282.0585</td>\n",
       "      <td>58.818259</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>147.2020</td>\n",
       "      <td>37.496792</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>214.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>74.6270</td>\n",
       "      <td>37.605938</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>269.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp   wdir  wspd    pres\n",
       "0     232.1240   59.746988  18.7  15.5  22.6   0.0   75.0  12.5  1023.9\n",
       "1     168.0445   73.870523  18.6  13.9  23.4   0.0   45.0  10.1  1021.8\n",
       "2     194.0030   59.394005  19.2  13.7  24.4   0.0   52.0   8.4  1021.5\n",
       "3     343.2730   68.192323  20.6  15.7  26.8   0.0  135.0  10.1  1021.8\n",
       "4     190.1570   78.645600  21.8  14.9  27.8   0.0  177.0  10.2  1020.0\n",
       "..         ...         ...   ...   ...   ...   ...    ...   ...     ...\n",
       "772    85.2440   40.983786   3.2   0.1   5.6   0.5   89.0  11.3  1019.5\n",
       "773   163.9400   37.200143  -0.1  -1.7   2.1   0.0  277.0  10.6  1018.6\n",
       "774   282.0585   58.818259   0.1  -2.1   2.6   0.0  178.0   6.6  1026.4\n",
       "775   147.2020   37.496792   4.8  -0.8   8.3   0.5  214.0  14.8  1020.0\n",
       "776    74.6270   37.605938   5.3   2.2   8.1   5.3  269.0  12.8  1023.6\n",
       "\n",
       "[581 rows x 9 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "# ensure all data is float\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[10,11,12,13,14,15,16,17,]], axis=1, inplace=True)\n",
    "# print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var6(t-1)</th>\n",
       "      <th>var7(t-1)</th>\n",
       "      <th>var8(t-1)</th>\n",
       "      <th>var9(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.435560</td>\n",
       "      <td>0.143119</td>\n",
       "      <td>0.657439</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182663</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.660969</td>\n",
       "      <td>0.347648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.347648</td>\n",
       "      <td>0.181692</td>\n",
       "      <td>0.653979</td>\n",
       "      <td>0.757937</td>\n",
       "      <td>0.765487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089783</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>0.631054</td>\n",
       "      <td>0.383261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383261</td>\n",
       "      <td>0.142155</td>\n",
       "      <td>0.674740</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.809735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111455</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.626781</td>\n",
       "      <td>0.588048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.588048</td>\n",
       "      <td>0.166184</td>\n",
       "      <td>0.723183</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.889381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>0.631054</td>\n",
       "      <td>0.377984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.377984</td>\n",
       "      <td>0.194734</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.915929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498452</td>\n",
       "      <td>0.187675</td>\n",
       "      <td>0.605413</td>\n",
       "      <td>0.486563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0.180355</td>\n",
       "      <td>0.048106</td>\n",
       "      <td>0.121107</td>\n",
       "      <td>0.281746</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358543</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.234052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>0.234052</td>\n",
       "      <td>0.091874</td>\n",
       "      <td>0.121107</td>\n",
       "      <td>0.210317</td>\n",
       "      <td>0.066372</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.226006</td>\n",
       "      <td>0.218487</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>0.342017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0.342017</td>\n",
       "      <td>0.081541</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.808050</td>\n",
       "      <td>0.198880</td>\n",
       "      <td>0.585470</td>\n",
       "      <td>0.504066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0.504066</td>\n",
       "      <td>0.140583</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.123016</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501548</td>\n",
       "      <td>0.086835</td>\n",
       "      <td>0.696581</td>\n",
       "      <td>0.319053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.319053</td>\n",
       "      <td>0.082351</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.613003</td>\n",
       "      <td>0.316527</td>\n",
       "      <td>0.605413</td>\n",
       "      <td>0.219486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
       "1     0.435560   0.143119   0.657439   0.821429   0.734513   0.000000   \n",
       "2     0.347648   0.181692   0.653979   0.757937   0.765487   0.000000   \n",
       "3     0.383261   0.142155   0.674740   0.750000   0.809735   0.000000   \n",
       "4     0.588048   0.166184   0.723183   0.829365   0.889381   0.000000   \n",
       "5     0.377984   0.194734   0.764706   0.797619   0.915929   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "576   0.180355   0.048106   0.121107   0.281746   0.035398   0.000000   \n",
       "577   0.234052   0.091874   0.121107   0.210317   0.066372   0.014706   \n",
       "578   0.342017   0.081541   0.006920   0.138889   0.000000   0.000000   \n",
       "579   0.504066   0.140583   0.013841   0.123016   0.013274   0.000000   \n",
       "580   0.319053   0.082351   0.176471   0.174603   0.163717   0.014706   \n",
       "\n",
       "     var7(t-1)  var8(t-1)  var9(t-1)   var1(t)  \n",
       "1     0.182663   0.252101   0.660969  0.347648  \n",
       "2     0.089783   0.184874   0.631054  0.383261  \n",
       "3     0.111455   0.137255   0.626781  0.588048  \n",
       "4     0.368421   0.184874   0.631054  0.377984  \n",
       "5     0.498452   0.187675   0.605413  0.486563  \n",
       "..         ...        ...        ...       ...  \n",
       "576   0.000000   0.358543   0.606838  0.234052  \n",
       "577   0.226006   0.218487   0.598291  0.342017  \n",
       "578   0.808050   0.198880   0.585470  0.504066  \n",
       "579   0.501548   0.086835   0.696581  0.319053  \n",
       "580   0.613003   0.316527   0.605413  0.219486  \n",
       "\n",
       "[580 rows x 10 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var6(t-1)</th>\n",
       "      <th>var7(t-1)</th>\n",
       "      <th>var8(t-1)</th>\n",
       "      <th>var9(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.435560</td>\n",
       "      <td>0.143119</td>\n",
       "      <td>0.657439</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182663</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.660969</td>\n",
       "      <td>0.347648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.347648</td>\n",
       "      <td>0.181692</td>\n",
       "      <td>0.653979</td>\n",
       "      <td>0.757937</td>\n",
       "      <td>0.765487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089783</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>0.631054</td>\n",
       "      <td>0.383261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383261</td>\n",
       "      <td>0.142155</td>\n",
       "      <td>0.674740</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.809735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111455</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.626781</td>\n",
       "      <td>0.588048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.588048</td>\n",
       "      <td>0.166184</td>\n",
       "      <td>0.723183</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.889381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>0.631054</td>\n",
       "      <td>0.377984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.377984</td>\n",
       "      <td>0.194734</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.915929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498452</td>\n",
       "      <td>0.187675</td>\n",
       "      <td>0.605413</td>\n",
       "      <td>0.486563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
       "1   0.435560   0.143119   0.657439   0.821429   0.734513        0.0   \n",
       "2   0.347648   0.181692   0.653979   0.757937   0.765487        0.0   \n",
       "3   0.383261   0.142155   0.674740   0.750000   0.809735        0.0   \n",
       "4   0.588048   0.166184   0.723183   0.829365   0.889381        0.0   \n",
       "5   0.377984   0.194734   0.764706   0.797619   0.915929        0.0   \n",
       "\n",
       "   var7(t-1)  var8(t-1)  var9(t-1)   var1(t)  \n",
       "1   0.182663   0.252101   0.660969  0.347648  \n",
       "2   0.089783   0.184874   0.631054  0.383261  \n",
       "3   0.111455   0.137255   0.626781  0.588048  \n",
       "4   0.368421   0.184874   0.631054  0.377984  \n",
       "5   0.498452   0.187675   0.605413  0.486563  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 1, 9) (406,) (174, 1, 9) (174,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "#70% training data\n",
    "n_train_hours = 406\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.2562 - val_loss: 0.1288\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.1377 - val_loss: 0.0732\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.1058 - val_loss: 0.0863\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.1065 - val_loss: 0.0809\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.0989 - val_loss: 0.0701\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0953 - val_loss: 0.0662\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0940 - val_loss: 0.0658\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0917 - val_loss: 0.0660\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0895 - val_loss: 0.0655\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0878 - val_loss: 0.0643\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0863 - val_loss: 0.0635\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0849 - val_loss: 0.0633\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.0837 - val_loss: 0.0622\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0828 - val_loss: 0.0624\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0821 - val_loss: 0.0626\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0816 - val_loss: 0.0624\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0811 - val_loss: 0.0623\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0807 - val_loss: 0.0631\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0805 - val_loss: 0.0633\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0802 - val_loss: 0.0633\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0800 - val_loss: 0.0636\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0797 - val_loss: 0.0639\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0796 - val_loss: 0.0645\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0796 - val_loss: 0.0641\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0794 - val_loss: 0.0644\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0795 - val_loss: 0.0647\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0793 - val_loss: 0.0644\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0793 - val_loss: 0.0647\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0792 - val_loss: 0.0651\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0792 - val_loss: 0.0648\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0791 - val_loss: 0.0654\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0791 - val_loss: 0.0649\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0790 - val_loss: 0.0654\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0790 - val_loss: 0.0652\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0789 - val_loss: 0.0652\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0789 - val_loss: 0.0652\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0789 - val_loss: 0.0647\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0787 - val_loss: 0.0648\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0789 - val_loss: 0.0649\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0785 - val_loss: 0.0642\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0788 - val_loss: 0.0655\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0784 - val_loss: 0.0640\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0787 - val_loss: 0.0663\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0784 - val_loss: 0.0648\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0784 - val_loss: 0.0656\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0784 - val_loss: 0.0648\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0782 - val_loss: 0.0650\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0786 - val_loss: 0.0651\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0782 - val_loss: 0.0647\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0786 - val_loss: 0.0660\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=50, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.22600264, 0.32470812, 0.2733564 , ..., 0.59133127,\n",
       "         0.37815126, 0.22364672]],\n",
       "\n",
       "       [[0.43460241, 0.47554698, 0.17647059, ..., 0.73684211,\n",
       "         0.08683473, 0.40883191]],\n",
       "\n",
       "       [[0.33132872, 0.25718367, 0.26989619, ..., 0.15170279,\n",
       "         0.18767507, 0.47150997]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.3420167 , 0.08154088, 0.00692042, ..., 0.80804954,\n",
       "         0.19887955, 0.58547009]],\n",
       "\n",
       "       [[0.50406638, 0.14058262, 0.01384083, ..., 0.50154799,\n",
       "         0.08683473, 0.6965812 ]],\n",
       "\n",
       "       [[0.31905343, 0.08235107, 0.17647059, ..., 0.6130031 ,\n",
       "         0.31652661, 0.60541311]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 62.347\n",
      "Mean absolute error: 48.12\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "print(\"Mean absolute error: %.2f\"\n",
    "      % mean_absolute_error(inv_y,inv_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tuning finding neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.013123 using {'n_neurons': 64}\n",
      "-0.014199 (0.004914) with: {'n_neurons': 5}\n",
      "-0.013920 (0.004838) with: {'n_neurons': 6}\n",
      "-0.013828 (0.004082) with: {'n_neurons': 7}\n",
      "-0.014158 (0.004408) with: {'n_neurons': 8}\n",
      "-0.013660 (0.004153) with: {'n_neurons': 9}\n",
      "-0.013877 (0.003956) with: {'n_neurons': 10}\n",
      "-0.014615 (0.005070) with: {'n_neurons': 15}\n",
      "-0.013827 (0.004333) with: {'n_neurons': 20}\n",
      "-0.013955 (0.003999) with: {'n_neurons': 25}\n",
      "-0.013371 (0.004076) with: {'n_neurons': 30}\n",
      "-0.014322 (0.003971) with: {'n_neurons': 35}\n",
      "-0.014225 (0.004962) with: {'n_neurons': 40}\n",
      "-0.013123 (0.003988) with: {'n_neurons': 64}\n",
      "-0.015248 (0.003206) with: {'n_neurons': 128}\n",
      "-0.013558 (0.004406) with: {'n_neurons': 256}\n"
     ]
    }
   ],
   "source": [
    "def create_model(n_neurons=1):\n",
    "    #create model\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(LSTM(n_neurons,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    # Compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons=[5,6,7,8,9, 10, 15, 20, 25, 30, 35,40,64,128,256]\n",
    "model = KerasRegressor(build_fn = create_model, epochs = 100, batch_size = 10, verbose = 0)\n",
    "#this does 3-fold classification. One can change k. \n",
    "param_grid = dict(n_neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Tuning finding batch size and epoch -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Epoch 1/200\n",
      "406/406 [==============================] - 1s 4ms/step - loss: 0.0981\n",
      "Epoch 2/200\n",
      "406/406 [==============================] - 0s 76us/step - loss: 0.0587\n",
      "Epoch 3/200\n",
      "406/406 [==============================] - 0s 64us/step - loss: 0.0339\n",
      "Epoch 4/200\n",
      "406/406 [==============================] - 0s 87us/step - loss: 0.0220\n",
      "Epoch 5/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0186\n",
      "Epoch 6/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0192\n",
      "Epoch 7/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0198\n",
      "Epoch 8/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0191\n",
      "Epoch 9/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0180\n",
      "Epoch 10/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0174\n",
      "Epoch 11/200\n",
      "406/406 [==============================] - 0s 57us/step - loss: 0.0174\n",
      "Epoch 12/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0174\n",
      "Epoch 13/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0173\n",
      "Epoch 14/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0168\n",
      "Epoch 15/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0166\n",
      "Epoch 16/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0164\n",
      "Epoch 17/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0162\n",
      "Epoch 18/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0160\n",
      "Epoch 19/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0159\n",
      "Epoch 20/200\n",
      "406/406 [==============================] - 0s 55us/step - loss: 0.0158\n",
      "Epoch 21/200\n",
      "406/406 [==============================] - 0s 67us/step - loss: 0.0156\n",
      "Epoch 22/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0154\n",
      "Epoch 23/200\n",
      "406/406 [==============================] - 0s 55us/step - loss: 0.0153\n",
      "Epoch 24/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0152\n",
      "Epoch 25/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0151\n",
      "Epoch 26/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0150\n",
      "Epoch 27/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0149\n",
      "Epoch 28/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0148\n",
      "Epoch 29/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0147\n",
      "Epoch 30/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0146\n",
      "Epoch 31/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0145\n",
      "Epoch 32/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0145\n",
      "Epoch 33/200\n",
      "406/406 [==============================] - 0s 60us/step - loss: 0.0145\n",
      "Epoch 34/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0144\n",
      "Epoch 35/200\n",
      "406/406 [==============================] - 0s 55us/step - loss: 0.0143\n",
      "Epoch 36/200\n",
      "406/406 [==============================] - 0s 63us/step - loss: 0.0142\n",
      "Epoch 37/200\n",
      "406/406 [==============================] - 0s 81us/step - loss: 0.0142\n",
      "Epoch 38/200\n",
      "406/406 [==============================] - 0s 81us/step - loss: 0.0141\n",
      "Epoch 39/200\n",
      "406/406 [==============================] - 0s 113us/step - loss: 0.0141\n",
      "Epoch 40/200\n",
      "406/406 [==============================] - 0s 174us/step - loss: 0.0141\n",
      "Epoch 41/200\n",
      "406/406 [==============================] - 0s 109us/step - loss: 0.0141\n",
      "Epoch 42/200\n",
      "406/406 [==============================] - 0s 74us/step - loss: 0.0139\n",
      "Epoch 43/200\n",
      "406/406 [==============================] - 0s 83us/step - loss: 0.0139\n",
      "Epoch 44/200\n",
      "406/406 [==============================] - 0s 78us/step - loss: 0.0140\n",
      "Epoch 45/200\n",
      "406/406 [==============================] - 0s 101us/step - loss: 0.0139\n",
      "Epoch 46/200\n",
      "406/406 [==============================] - 0s 73us/step - loss: 0.0138\n",
      "Epoch 47/200\n",
      "406/406 [==============================] - 0s 92us/step - loss: 0.0137\n",
      "Epoch 48/200\n",
      "406/406 [==============================] - 0s 90us/step - loss: 0.0137\n",
      "Epoch 49/200\n",
      "406/406 [==============================] - 0s 71us/step - loss: 0.0137\n",
      "Epoch 50/200\n",
      "406/406 [==============================] - 0s 81us/step - loss: 0.0136\n",
      "Epoch 51/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0137\n",
      "Epoch 52/200\n",
      "406/406 [==============================] - 0s 64us/step - loss: 0.0137\n",
      "Epoch 53/200\n",
      "406/406 [==============================] - 0s 74us/step - loss: 0.0136\n",
      "Epoch 54/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0136\n",
      "Epoch 55/200\n",
      "406/406 [==============================] - 0s 71us/step - loss: 0.0135\n",
      "Epoch 56/200\n",
      "406/406 [==============================] - 0s 68us/step - loss: 0.0135\n",
      "Epoch 57/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0135\n",
      "Epoch 58/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0135\n",
      "Epoch 59/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0136\n",
      "Epoch 60/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0135\n",
      "Epoch 61/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0135\n",
      "Epoch 62/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0135\n",
      "Epoch 63/200\n",
      "406/406 [==============================] - 0s 47us/step - loss: 0.0133\n",
      "Epoch 64/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0133\n",
      "Epoch 65/200\n",
      "406/406 [==============================] - 0s 81us/step - loss: 0.0133\n",
      "Epoch 66/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0133\n",
      "Epoch 67/200\n",
      "406/406 [==============================] - 0s 81us/step - loss: 0.0133\n",
      "Epoch 68/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0132\n",
      "Epoch 69/200\n",
      "406/406 [==============================] - 0s 132us/step - loss: 0.0133\n",
      "Epoch 70/200\n",
      "406/406 [==============================] - 0s 69us/step - loss: 0.0132\n",
      "Epoch 71/200\n",
      "406/406 [==============================] - 0s 96us/step - loss: 0.0132\n",
      "Epoch 72/200\n",
      "406/406 [==============================] - 0s 67us/step - loss: 0.0133\n",
      "Epoch 73/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0133\n",
      "Epoch 74/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0132\n",
      "Epoch 75/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0133\n",
      "Epoch 76/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0132\n",
      "Epoch 77/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0132\n",
      "Epoch 78/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0131\n",
      "Epoch 79/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0132\n",
      "Epoch 80/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0131\n",
      "Epoch 81/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0131\n",
      "Epoch 82/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0132\n",
      "Epoch 83/200\n",
      "406/406 [==============================] - 0s 58us/step - loss: 0.0132\n",
      "Epoch 84/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0131\n",
      "Epoch 85/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0132\n",
      "Epoch 86/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0130\n",
      "Epoch 87/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0131\n",
      "Epoch 88/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0131\n",
      "Epoch 89/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0131\n",
      "Epoch 90/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0130\n",
      "Epoch 91/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0130\n",
      "Epoch 92/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0131\n",
      "Epoch 93/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0130\n",
      "Epoch 94/200\n",
      "406/406 [==============================] - 0s 64us/step - loss: 0.0130\n",
      "Epoch 95/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0130\n",
      "Epoch 96/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0130\n",
      "Epoch 97/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0130\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406/406 [==============================] - 0s 51us/step - loss: 0.0130\n",
      "Epoch 99/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0130\n",
      "Epoch 100/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0131\n",
      "Epoch 101/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0129\n",
      "Epoch 102/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0129\n",
      "Epoch 103/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0129\n",
      "Epoch 104/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0129\n",
      "Epoch 105/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0130\n",
      "Epoch 106/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0131\n",
      "Epoch 107/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0129\n",
      "Epoch 108/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0130\n",
      "Epoch 109/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0132\n",
      "Epoch 110/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0129\n",
      "Epoch 111/200\n",
      "406/406 [==============================] - 0s 63us/step - loss: 0.0130\n",
      "Epoch 112/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0129\n",
      "Epoch 113/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0129\n",
      "Epoch 114/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0129\n",
      "Epoch 115/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0129\n",
      "Epoch 116/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0129\n",
      "Epoch 117/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0130\n",
      "Epoch 118/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0129\n",
      "Epoch 119/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0129\n",
      "Epoch 120/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0129\n",
      "Epoch 121/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0128\n",
      "Epoch 122/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0128\n",
      "Epoch 123/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0130\n",
      "Epoch 124/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0128\n",
      "Epoch 125/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0129\n",
      "Epoch 126/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0128\n",
      "Epoch 127/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0128\n",
      "Epoch 128/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0134\n",
      "Epoch 129/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0134\n",
      "Epoch 130/200\n",
      "406/406 [==============================] - 0s 93us/step - loss: 0.0129\n",
      "Epoch 131/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0128\n",
      "Epoch 132/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0128\n",
      "Epoch 133/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0128\n",
      "Epoch 134/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0128\n",
      "Epoch 135/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0128\n",
      "Epoch 136/200\n",
      "406/406 [==============================] - 0s 56us/step - loss: 0.0128\n",
      "Epoch 137/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0128\n",
      "Epoch 138/200\n",
      "406/406 [==============================] - 0s 76us/step - loss: 0.0127\n",
      "Epoch 139/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0128\n",
      "Epoch 140/200\n",
      "406/406 [==============================] - 0s 88us/step - loss: 0.0127\n",
      "Epoch 141/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0127\n",
      "Epoch 142/200\n",
      "406/406 [==============================] - 0s 61us/step - loss: 0.0128\n",
      "Epoch 143/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0128\n",
      "Epoch 144/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0128\n",
      "Epoch 145/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0132\n",
      "Epoch 146/200\n",
      "406/406 [==============================] - 0s 55us/step - loss: 0.0127\n",
      "Epoch 147/200\n",
      "406/406 [==============================] - 0s 57us/step - loss: 0.0127\n",
      "Epoch 148/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0127\n",
      "Epoch 149/200\n",
      "406/406 [==============================] - 0s 60us/step - loss: 0.0127\n",
      "Epoch 150/200\n",
      "406/406 [==============================] - 0s 58us/step - loss: 0.0128\n",
      "Epoch 151/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0127\n",
      "Epoch 152/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0127\n",
      "Epoch 153/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0128\n",
      "Epoch 154/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0127\n",
      "Epoch 155/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0127\n",
      "Epoch 156/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0127\n",
      "Epoch 157/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0127\n",
      "Epoch 158/200\n",
      "406/406 [==============================] - 0s 73us/step - loss: 0.0126\n",
      "Epoch 159/200\n",
      "406/406 [==============================] - 0s 90us/step - loss: 0.0127\n",
      "Epoch 160/200\n",
      "406/406 [==============================] - 0s 83us/step - loss: 0.0126\n",
      "Epoch 161/200\n",
      "406/406 [==============================] - 0s 73us/step - loss: 0.0127\n",
      "Epoch 162/200\n",
      "406/406 [==============================] - 0s 97us/step - loss: 0.0130\n",
      "Epoch 163/200\n",
      "406/406 [==============================] - 0s 78us/step - loss: 0.0126\n",
      "Epoch 164/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0129\n",
      "Epoch 165/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0128\n",
      "Epoch 166/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0126\n",
      "Epoch 167/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0126\n",
      "Epoch 168/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0126\n",
      "Epoch 169/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0126\n",
      "Epoch 170/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0126\n",
      "Epoch 171/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0127\n",
      "Epoch 172/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0126\n",
      "Epoch 173/200\n",
      "406/406 [==============================] - 0s 52us/step - loss: 0.0128\n",
      "Epoch 174/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0127\n",
      "Epoch 175/200\n",
      "406/406 [==============================] - 0s 47us/step - loss: 0.0127\n",
      "Epoch 176/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0127\n",
      "Epoch 177/200\n",
      "406/406 [==============================] - 0s 53us/step - loss: 0.0126\n",
      "Epoch 178/200\n",
      "406/406 [==============================] - 0s 91us/step - loss: 0.0125\n",
      "Epoch 179/200\n",
      "406/406 [==============================] - 0s 62us/step - loss: 0.0127\n",
      "Epoch 180/200\n",
      "406/406 [==============================] - 0s 88us/step - loss: 0.0127\n",
      "Epoch 181/200\n",
      "406/406 [==============================] - 0s 47us/step - loss: 0.0132\n",
      "Epoch 182/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0128\n",
      "Epoch 183/200\n",
      "406/406 [==============================] - 0s 55us/step - loss: 0.0130\n",
      "Epoch 184/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0127\n",
      "Epoch 185/200\n",
      "406/406 [==============================] - 0s 51us/step - loss: 0.0128\n",
      "Epoch 186/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0126\n",
      "Epoch 187/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0130\n",
      "Epoch 188/200\n",
      "406/406 [==============================] - 0s 47us/step - loss: 0.0129\n",
      "Epoch 189/200\n",
      "406/406 [==============================] - 0s 48us/step - loss: 0.0128\n",
      "Epoch 190/200\n",
      "406/406 [==============================] - 0s 50us/step - loss: 0.0126\n",
      "Epoch 191/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0126\n",
      "Epoch 192/200\n",
      "406/406 [==============================] - 0s 49us/step - loss: 0.0126\n",
      "Epoch 193/200\n",
      "406/406 [==============================] - 0s 65us/step - loss: 0.0126\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406/406 [==============================] - 0s 65us/step - loss: 0.0125\n",
      "Epoch 195/200\n",
      "406/406 [==============================] - 0s 69us/step - loss: 0.0131\n",
      "Epoch 196/200\n",
      "406/406 [==============================] - 0s 59us/step - loss: 0.0127\n",
      "Epoch 197/200\n",
      "406/406 [==============================] - 0s 54us/step - loss: 0.0127\n",
      "Epoch 198/200\n",
      "406/406 [==============================] - 0s 57us/step - loss: 0.0127\n",
      "Epoch 199/200\n",
      "406/406 [==============================] - 0s 76us/step - loss: 0.0126\n",
      "Epoch 200/200\n",
      "406/406 [==============================] - 0s 80us/step - loss: 0.0126\n",
      "Best: -0.013895 using {'batch_size': 80, 'epochs': 200}\n",
      "-0.013941 (0.004952) with: {'batch_size': 50, 'epochs': 50}\n",
      "-0.013944 (0.004664) with: {'batch_size': 50, 'epochs': 100}\n",
      "-0.014222 (0.004362) with: {'batch_size': 50, 'epochs': 150}\n",
      "-0.014034 (0.004427) with: {'batch_size': 50, 'epochs': 200}\n",
      "-0.014154 (0.004794) with: {'batch_size': 60, 'epochs': 50}\n",
      "-0.013993 (0.004841) with: {'batch_size': 60, 'epochs': 100}\n",
      "-0.013999 (0.004666) with: {'batch_size': 60, 'epochs': 150}\n",
      "-0.014112 (0.004367) with: {'batch_size': 60, 'epochs': 200}\n",
      "-0.014386 (0.004988) with: {'batch_size': 70, 'epochs': 50}\n",
      "-0.013897 (0.004877) with: {'batch_size': 70, 'epochs': 100}\n",
      "-0.014039 (0.004726) with: {'batch_size': 70, 'epochs': 150}\n",
      "-0.013938 (0.004506) with: {'batch_size': 70, 'epochs': 200}\n",
      "-0.015028 (0.004601) with: {'batch_size': 80, 'epochs': 50}\n",
      "-0.014196 (0.004705) with: {'batch_size': 80, 'epochs': 100}\n",
      "-0.014038 (0.004921) with: {'batch_size': 80, 'epochs': 150}\n",
      "-0.013895 (0.004304) with: {'batch_size': 80, 'epochs': 200}\n",
      "-0.014801 (0.004681) with: {'batch_size': 100, 'epochs': 50}\n",
      "-0.013999 (0.004724) with: {'batch_size': 100, 'epochs': 100}\n",
      "-0.013908 (0.004749) with: {'batch_size': 100, 'epochs': 150}\n",
      "-0.013976 (0.004420) with: {'batch_size': 100, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "##############################################################\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "##############################################################\n",
    "# load dataset\n",
    "import pandas as pd\n",
    "\n",
    "input_dim = 8 # number of columns\n",
    "print(input_dim) # 23\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # default values\n",
    "    activation='relu' # or linear\n",
    "    dropout_rate=0.0 # or 0.2\n",
    "    init_mode='uniform'\n",
    "    weight_constraint=0 # or  4\n",
    "    optimizer='adam' # or SGD\n",
    "    lr = 0.01\n",
    "    momemntum=0\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(64,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer, \n",
    "               \n",
    "                  )\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, batch_size=100, epochs=100) \n",
    "# use verbose=0 if you do not want to see progress\n",
    "\n",
    "########################################################\n",
    "# Use scikit-learn to grid search \n",
    "activation =  ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'] # softmax, softplus, softsign \n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "weight_constraint=[1, 2, 3, 4, 5]\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "sizes = [1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "optimizer = [ 'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "##############################################################\n",
    "# grid search epochs, batch size\n",
    "epochs = [50,100,150,200] # add 50, 100, 150 etc\n",
    "batch_size = [50,60,70,80,100 ] # add 5, 10, 20, 40, 60, 80, 100 etc\n",
    "param_grid = dict(epochs=epochs, batch_size=batch_size)\n",
    "##############################################################\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(train_X, train_y) \n",
    "##############################################################\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: -0.013895 using {'batch_size': 80, 'epochs': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.51818741637405"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error( inv_yhat,inv_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
