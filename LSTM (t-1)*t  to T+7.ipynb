{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score , classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first load the data and take a look at what we have.\n",
    "df = pd.read_csv('heathrow_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], \n",
    "               axis=1,\n",
    "              inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>Hum</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.12</td>\n",
       "      <td>59.75</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>58.67</td>\n",
       "      <td>21.35</td>\n",
       "      <td>27.00</td>\n",
       "      <td>12.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.04</td>\n",
       "      <td>73.87</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.83</td>\n",
       "      <td>25.76</td>\n",
       "      <td>34.37</td>\n",
       "      <td>14.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.00</td>\n",
       "      <td>59.39</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "      <td>65.33</td>\n",
       "      <td>15.23</td>\n",
       "      <td>36.05</td>\n",
       "      <td>20.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.27</td>\n",
       "      <td>68.19</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.00</td>\n",
       "      <td>16.71</td>\n",
       "      <td>42.57</td>\n",
       "      <td>22.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.16</td>\n",
       "      <td>78.65</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>59.58</td>\n",
       "      <td>26.03</td>\n",
       "      <td>38.74</td>\n",
       "      <td>17.279167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>85.24</td>\n",
       "      <td>40.98</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>84.79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>25.35</td>\n",
       "      <td>7.420833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>163.94</td>\n",
       "      <td>37.20</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "      <td>91.63</td>\n",
       "      <td>6.15</td>\n",
       "      <td>27.77</td>\n",
       "      <td>15.304167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>282.06</td>\n",
       "      <td>58.82</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "      <td>94.25</td>\n",
       "      <td>17.17</td>\n",
       "      <td>32.50</td>\n",
       "      <td>13.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>147.20</td>\n",
       "      <td>37.50</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>86.63</td>\n",
       "      <td>8.21</td>\n",
       "      <td>25.78</td>\n",
       "      <td>6.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>74.63</td>\n",
       "      <td>37.61</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "      <td>78.79</td>\n",
       "      <td>8.37</td>\n",
       "      <td>25.99</td>\n",
       "      <td>9.929167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp  wdir  wspd    pres    Hum  \\\n",
       "0       232.12       59.75  18.7  15.5  22.6   0.0     2  12.5  1023.9  58.67   \n",
       "1       168.04       73.87  18.6  13.9  23.4   0.0     1  10.1  1021.8  65.83   \n",
       "2       194.00       59.39  19.2  13.7  24.4   0.0     2   8.4  1021.5  65.33   \n",
       "3       343.27       68.19  20.6  15.7  26.8   0.0     2  10.1  1021.8  65.00   \n",
       "4       190.16       78.65  21.8  14.9  27.8   0.0     3  10.2  1020.0  59.58   \n",
       "..         ...         ...   ...   ...   ...   ...   ...   ...     ...    ...   \n",
       "575      85.24       40.98   3.2   0.1   5.6   0.5     2  11.3  1019.5  84.79   \n",
       "576     163.94       37.20  -0.1  -1.7   2.1   0.0     4  10.6  1018.6  91.63   \n",
       "577     282.06       58.82   0.1  -2.1   2.6   0.0     3   6.6  1026.4  94.25   \n",
       "578     147.20       37.50   4.8  -0.8   8.3   0.5     3  14.8  1020.0  86.63   \n",
       "579      74.63       37.61   5.3   2.2   8.1   5.3     4  12.8  1023.6  78.79   \n",
       "\n",
       "        NO    NO2       PM10  \n",
       "0    21.35  27.00  12.395833  \n",
       "1    25.76  34.37  14.937500  \n",
       "2    15.23  36.05  20.891667  \n",
       "3    16.71  42.57  22.316667  \n",
       "4    26.03  38.74  17.279167  \n",
       "..     ...    ...        ...  \n",
       "575  10.20  25.35   7.420833  \n",
       "576   6.15  27.77  15.304167  \n",
       "577  17.17  32.50  13.537500  \n",
       "578   8.21  25.78   6.412500  \n",
       "579   8.37  25.99   9.929167  \n",
       "\n",
       "[577 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Nox_tropo'], axis=1).values\n",
    "y = df['Nox_tropo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "n_hours = 1\n",
    "n_features = 13\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours , 8)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,\n",
    "                               52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103]], axis=1, inplace=True)\n",
    "# print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)',\n",
       "       'var6(t-1)', 'var7(t-1)', 'var8(t-1)', 'var9(t-1)', 'var10(t-1)',\n",
       "       'var11(t-1)', 'var12(t-1)', 'var13(t-1)', 'var1(t+7)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 13) 460 (460,)\n",
      "(460, 1, 13) (460,) (109, 1, 13) (109,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "#80% training data\n",
    "n_train_hours = 460\n",
    "train = values[:n_train_hours]\n",
    "test = values[n_train_hours:]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# train = data.values[:459]\n",
    "# test = data.values[459:]\n",
    "\n",
    "# # Separate input and output\n",
    "# train_X, train_y = train[:, :-1], train[:, -1]\n",
    "# test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# # Reshape input to be 3D [samples, timesteps, features]\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "# # Print all shapes\n",
    "# train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14313806, 0.18170149, 0.14215485, 0.16618872, 0.19475625,\n",
       "       0.13737539, 0.1449406 , 0.10468387, 0.10990031, 0.1327598 ,\n",
       "       0.2962174 , 0.24296054, 0.13729346, 0.3498293 , 0.3001229 ,\n",
       "       0.20092858, 0.20464291, 0.22698348, 0.16657108, 0.19117848,\n",
       "       0.14808139, 0.2341117 , 0.07936638, 0.29602622, 0.22714734,\n",
       "       0.20849379, 0.25874642, 0.10301789, 0.108644  , 0.17866994,\n",
       "       0.201229  , 0.15515499, 0.18735491, 0.16733579, 0.17017616,\n",
       "       0.2211935 , 0.21627748, 0.12265465, 0.1517684 , 0.19554827,\n",
       "       0.26210569, 0.12503073, 0.20546224, 0.16231053, 0.30725113,\n",
       "       0.27783695, 0.35821385, 0.27393145, 0.39912604, 0.17858801,\n",
       "       0.22335109, 0.17823296, 0.27368565, 0.17869726, 0.16501434,\n",
       "       0.18940325, 0.34297419, 0.19309026, 0.16741772, 0.22698348,\n",
       "       0.1809914 , 0.18153762, 0.20294961, 0.14488598, 0.18915745,\n",
       "       0.45647958, 0.58989485, 0.37072238, 0.42138468, 0.3415267 ,\n",
       "       0.50293596, 0.16476854, 0.20297692, 0.10301789, 0.33497201,\n",
       "       0.30132459, 0.35938823, 0.19063225, 0.32322819, 0.30686877,\n",
       "       0.40379626, 0.48586645, 0.10484774, 0.22255906, 0.23383859,\n",
       "       0.26005735, 0.25809095, 0.24888707, 0.57659429, 0.49755565,\n",
       "       0.26033047, 0.34570531, 0.25361191, 0.16195548, 0.14925577,\n",
       "       0.15938823, 0.17998088, 0.26541035, 0.24170422, 0.39038645,\n",
       "       0.31945924, 0.47100915, 0.27095453, 0.06978014, 0.54677045,\n",
       "       0.14682507, 0.25495016, 0.18678137, 0.11541718, 0.25241021,\n",
       "       0.26614775, 0.30449269, 0.29649051, 0.313997  , 0.13939642,\n",
       "       0.43820838, 0.30793391, 0.29236652, 0.26341663, 0.96621603,\n",
       "       0.39139697, 0.37640311, 0.33092995, 0.39792435, 0.20581729,\n",
       "       0.20955892, 0.21272702, 0.32858118, 0.25915608, 0.21854431,\n",
       "       0.27278438, 0.45817288, 0.26278847, 0.13103919, 0.1639492 ,\n",
       "       0.08878875, 0.2353134 , 0.54955619, 0.10886249, 0.26429059,\n",
       "       0.5571214 , 0.38118258, 0.08728663, 0.30949065, 0.2779462 ,\n",
       "       0.17550184, 0.22780281, 0.22911375, 0.6784378 , 0.40464291,\n",
       "       0.39145159, 0.14622423, 0.1213164 , 0.11467978, 0.12718831,\n",
       "       0.21466612, 0.27313942, 0.3193773 , 0.16703537, 0.19991807,\n",
       "       0.16465929, 0.15504575, 0.1965861 , 0.38336747, 0.30583094,\n",
       "       0.07111839, 0.16433156, 0.25077154, 0.23080705, 0.26978014,\n",
       "       0.35037553, 0.07057217, 0.57277072, 0.56648914, 0.34898266,\n",
       "       0.4163321 , 0.73046566, 0.29599891, 0.19199782, 0.05697119,\n",
       "       0.24760344, 0.35671173, 0.57659429, 0.85582412, 0.37653967,\n",
       "       0.14483135, 0.34879148, 0.40944968, 0.26420866, 0.32503073,\n",
       "       0.22064728, 0.21040557, 0.20163867, 0.15903318, 0.36405845,\n",
       "       0.34671583, 0.28816059, 0.79109655, 0.51549911, 0.2830807 ,\n",
       "       0.25606992, 0.28758705, 0.26969821, 0.25232828, 0.29908507,\n",
       "       0.41600437, 0.30727844, 0.79948109, 1.        , 0.86155947,\n",
       "       0.2521371 , 0.19606719, 0.15261505, 0.21674177, 0.18705449,\n",
       "       0.22687423, 0.22695617, 0.12186262, 0.24926943, 0.17214256,\n",
       "       0.15220538, 0.09796531, 0.12309163, 0.05784515, 0.10143384,\n",
       "       0.09788338, 0.14084392, 0.10553052, 0.26860576, 0.25145432,\n",
       "       0.19527516, 0.16561519, 0.23170832, 0.20928581, 0.21133415,\n",
       "       0.29392326, 0.42223133, 0.38650826, 0.35482726, 0.54171788,\n",
       "       0.17684009, 0.03687014, 0.06224225, 0.32251809, 0.30148846,\n",
       "       0.43891848, 0.40904001, 0.27262051, 0.23443944, 0.13863171,\n",
       "       0.24047522, 0.18459648, 0.34633347, 0.56599754, 0.19451045,\n",
       "       0.08302608, 0.13906869, 0.26562884, 0.15307934, 0.23648778,\n",
       "       0.14540489, 0.16790933, 0.07906596, 0.17353544, 0.1919705 ,\n",
       "       0.24976103, 0.13456234, 0.34846374, 0.33994265, 0.34210023,\n",
       "       0.29946743, 0.2730848 , 0.07445036, 0.14764441, 0.08512905,\n",
       "       0.16266557, 0.15182302, 0.20398744, 0.17667623, 0.1371569 ,\n",
       "       0.13041103, 0.12680595, 0.12656015, 0.14321999, 0.14791752,\n",
       "       0.13338796, 0.23124403, 0.25178206, 0.2255906 , 0.19833402,\n",
       "       0.17815103, 0.11508944, 0.09173836, 0.03809914, 0.35548273,\n",
       "       0.13150348, 0.12576813, 0.08351768, 0.22545405, 0.09949474,\n",
       "       0.05765397, 0.15081251, 0.17487369, 0.13865902, 0.06926123,\n",
       "       0.07180117, 0.17785061, 0.10039601, 0.24670217, 0.10301789,\n",
       "       0.10178888, 0.18921207, 0.13581865, 0.18735491, 0.12636897,\n",
       "       0.24763075, 0.1593063 , 0.17836952, 0.2285129 , 0.10370067,\n",
       "       0.37208794, 0.17451864, 0.1727161 , 0.1851427 , 0.05404889,\n",
       "       0.17591151, 0.08499249, 0.14013382, 0.08267104, 0.26000273,\n",
       "       0.1727161 , 0.19617643, 0.18631708, 0.12079749, 0.20051891,\n",
       "       0.21021439, 0.21537621, 0.0867131 , 0.24386181, 0.09690018,\n",
       "       0.15993445, 0.1897583 , 0.18970367, 0.16766353, 0.27753653,\n",
       "       0.22821248, 0.36239246, 0.17460057, 0.14185443, 0.19377304,\n",
       "       0.17815103, 0.0911102 , 0.20505257, 0.18519732, 0.09520688,\n",
       "       0.13625563, 0.18669944, 0.14783559, 0.23550457, 0.14636078,\n",
       "       0.19795166, 0.18334016, 0.19336338, 0.17050389, 0.25544176,\n",
       "       0.18852929, 0.14548682, 0.25241021, 0.28100505, 0.3047385 ,\n",
       "       0.31697392, 0.14018845, 0.29474259, 0.21084255, 0.16547863,\n",
       "       0.20928581, 0.15777687, 0.18129182, 0.18899358, 0.14499522,\n",
       "       0.06439984, 0.20371432, 0.15441759, 0.19907142, 0.26808685,\n",
       "       0.27398607, 0.23252765, 0.30441076, 0.24375256, 0.31500751,\n",
       "       0.22900451, 0.2090127 , 0.40854841, 0.28816059, 0.46336201,\n",
       "       0.32582275, 0.29326779, 0.17470982, 0.26642087, 0.3264236 ,\n",
       "       0.52757067, 0.25393964, 0.3247303 , 0.47557012, 0.25718968,\n",
       "       0.1719787 , 0.19811553, 0.53652875, 0.69534344, 0.36302062,\n",
       "       0.39300833, 0.21280896, 0.14510447, 0.55135873, 0.70105148,\n",
       "       0.12590468, 0.40928581, 0.46565615, 0.47264782, 0.20546224,\n",
       "       0.26117711, 0.17673085, 0.20065547, 0.28835177, 0.23965588,\n",
       "       0.23842687, 0.23747098, 0.40046429, 0.2277755 , 0.18973098,\n",
       "       0.2684419 , 0.15654786, 0.30981838, 0.29862078, 0.24981565,\n",
       "       0.29094633, 0.27592517, 0.13024717, 0.20027311, 0.25440393,\n",
       "       0.21581319, 0.20284037, 0.25888297, 0.23023351, 0.22458009,\n",
       "       0.15157722, 0.26677591, 0.66677591, 0.67849242, 0.47106377,\n",
       "       0.24383449, 0.3705312 , 0.10146115, 0.13969685, 0.20079203,\n",
       "       0.1476171 , 0.14808139, 0.18110064, 0.08813328, 0.10411034])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 - 3s - loss: 0.0146 - val_loss: 0.0269\n",
      "Epoch 2/50\n",
      "154/154 - 0s - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 3/50\n",
      "154/154 - 0s - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 4/50\n",
      "154/154 - 0s - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 5/50\n",
      "154/154 - 0s - loss: 8.8435e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "154/154 - 0s - loss: 7.2948e-04 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "154/154 - 0s - loss: 5.8782e-04 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "154/154 - 0s - loss: 4.9040e-04 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "154/154 - 0s - loss: 4.1302e-04 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "154/154 - 0s - loss: 3.5986e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "154/154 - 0s - loss: 3.2157e-04 - val_loss: 0.0011\n",
      "Epoch 12/50\n",
      "154/154 - 0s - loss: 2.8872e-04 - val_loss: 9.9384e-04\n",
      "Epoch 13/50\n",
      "154/154 - 0s - loss: 2.7002e-04 - val_loss: 9.4458e-04\n",
      "Epoch 14/50\n",
      "154/154 - 0s - loss: 2.5286e-04 - val_loss: 8.8531e-04\n",
      "Epoch 15/50\n",
      "154/154 - 0s - loss: 2.3644e-04 - val_loss: 8.1529e-04\n",
      "Epoch 16/50\n",
      "154/154 - 0s - loss: 2.2423e-04 - val_loss: 7.7482e-04\n",
      "Epoch 17/50\n",
      "154/154 - 0s - loss: 2.1072e-04 - val_loss: 7.3295e-04\n",
      "Epoch 18/50\n",
      "154/154 - 0s - loss: 1.9897e-04 - val_loss: 6.8413e-04\n",
      "Epoch 19/50\n",
      "154/154 - 0s - loss: 1.8951e-04 - val_loss: 6.5040e-04\n",
      "Epoch 20/50\n",
      "154/154 - 0s - loss: 1.8131e-04 - val_loss: 6.3197e-04\n",
      "Epoch 21/50\n",
      "154/154 - 0s - loss: 1.7157e-04 - val_loss: 6.4493e-04\n",
      "Epoch 22/50\n",
      "154/154 - 0s - loss: 1.6163e-04 - val_loss: 6.1263e-04\n",
      "Epoch 23/50\n",
      "154/154 - 0s - loss: 1.6150e-04 - val_loss: 6.2127e-04\n",
      "Epoch 24/50\n",
      "154/154 - 0s - loss: 1.5115e-04 - val_loss: 5.9917e-04\n",
      "Epoch 25/50\n",
      "154/154 - 0s - loss: 1.4442e-04 - val_loss: 6.1053e-04\n",
      "Epoch 26/50\n",
      "154/154 - 0s - loss: 1.3779e-04 - val_loss: 5.7858e-04\n",
      "Epoch 27/50\n",
      "154/154 - 0s - loss: 1.3044e-04 - val_loss: 5.5141e-04\n",
      "Epoch 28/50\n",
      "154/154 - 0s - loss: 1.2733e-04 - val_loss: 5.0625e-04\n",
      "Epoch 29/50\n",
      "154/154 - 0s - loss: 1.1937e-04 - val_loss: 4.7012e-04\n",
      "Epoch 30/50\n",
      "154/154 - 0s - loss: 1.1430e-04 - val_loss: 4.4679e-04\n",
      "Epoch 31/50\n",
      "154/154 - 0s - loss: 1.0847e-04 - val_loss: 3.9841e-04\n",
      "Epoch 32/50\n",
      "154/154 - 1s - loss: 1.0672e-04 - val_loss: 3.8982e-04\n",
      "Epoch 33/50\n",
      "154/154 - 0s - loss: 1.0194e-04 - val_loss: 3.3231e-04\n",
      "Epoch 34/50\n",
      "154/154 - 0s - loss: 9.6749e-05 - val_loss: 3.2476e-04\n",
      "Epoch 35/50\n",
      "154/154 - 0s - loss: 9.2191e-05 - val_loss: 2.9472e-04\n",
      "Epoch 36/50\n",
      "154/154 - 0s - loss: 9.6047e-05 - val_loss: 2.7109e-04\n",
      "Epoch 37/50\n",
      "154/154 - 0s - loss: 9.2692e-05 - val_loss: 2.5440e-04\n",
      "Epoch 38/50\n",
      "154/154 - 0s - loss: 8.8172e-05 - val_loss: 2.4440e-04\n",
      "Epoch 39/50\n",
      "154/154 - 0s - loss: 7.9681e-05 - val_loss: 2.0662e-04\n",
      "Epoch 40/50\n",
      "154/154 - 0s - loss: 9.1759e-05 - val_loss: 2.1068e-04\n",
      "Epoch 41/50\n",
      "154/154 - 0s - loss: 7.1392e-05 - val_loss: 1.8993e-04\n",
      "Epoch 42/50\n",
      "154/154 - 0s - loss: 7.4510e-05 - val_loss: 1.7462e-04\n",
      "Epoch 43/50\n",
      "154/154 - 0s - loss: 8.2113e-05 - val_loss: 1.7222e-04\n",
      "Epoch 44/50\n",
      "154/154 - 0s - loss: 7.4799e-05 - val_loss: 1.5818e-04\n",
      "Epoch 45/50\n",
      "154/154 - 0s - loss: 7.7278e-05 - val_loss: 1.5672e-04\n",
      "Epoch 46/50\n",
      "154/154 - 0s - loss: 6.8028e-05 - val_loss: 1.5048e-04\n",
      "Epoch 47/50\n",
      "154/154 - 0s - loss: 6.8866e-05 - val_loss: 1.4592e-04\n",
      "Epoch 48/50\n",
      "154/154 - 0s - loss: 6.1792e-05 - val_loss: 1.4267e-04\n",
      "Epoch 49/50\n",
      "154/154 - 0s - loss: 5.2822e-05 - val_loss: 1.4309e-04\n",
      "Epoch 50/50\n",
      "154/154 - 0s - loss: 5.1595e-05 - val_loss: 1.3547e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(13, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu'))\n",
    "model.add(Dense(38, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 7.456\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -12:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -12:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.353112068719568"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.76286115041036"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13-38-1 Test RMSE: 62.006\n",
    "\n",
    "13-68-1 Test RMSE: 68.752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 61-1 Test RMSE: Test RMSE: 63.163\n",
    "\n",
    "13-40-1 Test RMSE: 67.366\n",
    "\n",
    "13-100-1 Test RMSE: 65.965\n",
    "\n",
    "13- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
