{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score , classification_report, mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first load the data and take a look at what we have.\n",
    "df = pd.read_csv('heathrow_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date'], \n",
    "               axis=1,\n",
    "              inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nox_tropo</th>\n",
       "      <th>Nox_ground</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>Hum</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232.12</td>\n",
       "      <td>59.75</td>\n",
       "      <td>18.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>58.67</td>\n",
       "      <td>21.35</td>\n",
       "      <td>27.00</td>\n",
       "      <td>12.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.04</td>\n",
       "      <td>73.87</td>\n",
       "      <td>18.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.83</td>\n",
       "      <td>25.76</td>\n",
       "      <td>34.37</td>\n",
       "      <td>14.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.00</td>\n",
       "      <td>59.39</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1021.5</td>\n",
       "      <td>65.33</td>\n",
       "      <td>15.23</td>\n",
       "      <td>36.05</td>\n",
       "      <td>20.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.27</td>\n",
       "      <td>68.19</td>\n",
       "      <td>20.6</td>\n",
       "      <td>15.7</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>65.00</td>\n",
       "      <td>16.71</td>\n",
       "      <td>42.57</td>\n",
       "      <td>22.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190.16</td>\n",
       "      <td>78.65</td>\n",
       "      <td>21.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>59.58</td>\n",
       "      <td>26.03</td>\n",
       "      <td>38.74</td>\n",
       "      <td>17.279167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>85.24</td>\n",
       "      <td>40.98</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>84.79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>25.35</td>\n",
       "      <td>7.420833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>163.94</td>\n",
       "      <td>37.20</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1018.6</td>\n",
       "      <td>91.63</td>\n",
       "      <td>6.15</td>\n",
       "      <td>27.77</td>\n",
       "      <td>15.304167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>282.06</td>\n",
       "      <td>58.82</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1026.4</td>\n",
       "      <td>94.25</td>\n",
       "      <td>17.17</td>\n",
       "      <td>32.50</td>\n",
       "      <td>13.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>147.20</td>\n",
       "      <td>37.50</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>86.63</td>\n",
       "      <td>8.21</td>\n",
       "      <td>25.78</td>\n",
       "      <td>6.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>74.63</td>\n",
       "      <td>37.61</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1023.6</td>\n",
       "      <td>78.79</td>\n",
       "      <td>8.37</td>\n",
       "      <td>25.99</td>\n",
       "      <td>9.929167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nox_tropo  Nox_ground  tavg  tmin  tmax  prcp  wdir  wspd    pres    Hum  \\\n",
       "0       232.12       59.75  18.7  15.5  22.6   0.0     2  12.5  1023.9  58.67   \n",
       "1       168.04       73.87  18.6  13.9  23.4   0.0     1  10.1  1021.8  65.83   \n",
       "2       194.00       59.39  19.2  13.7  24.4   0.0     2   8.4  1021.5  65.33   \n",
       "3       343.27       68.19  20.6  15.7  26.8   0.0     2  10.1  1021.8  65.00   \n",
       "4       190.16       78.65  21.8  14.9  27.8   0.0     3  10.2  1020.0  59.58   \n",
       "..         ...         ...   ...   ...   ...   ...   ...   ...     ...    ...   \n",
       "575      85.24       40.98   3.2   0.1   5.6   0.5     2  11.3  1019.5  84.79   \n",
       "576     163.94       37.20  -0.1  -1.7   2.1   0.0     4  10.6  1018.6  91.63   \n",
       "577     282.06       58.82   0.1  -2.1   2.6   0.0     3   6.6  1026.4  94.25   \n",
       "578     147.20       37.50   4.8  -0.8   8.3   0.5     3  14.8  1020.0  86.63   \n",
       "579      74.63       37.61   5.3   2.2   8.1   5.3     4  12.8  1023.6  78.79   \n",
       "\n",
       "        NO    NO2       PM10  \n",
       "0    21.35  27.00  12.395833  \n",
       "1    25.76  34.37  14.937500  \n",
       "2    15.23  36.05  20.891667  \n",
       "3    16.71  42.57  22.316667  \n",
       "4    26.03  38.74  17.279167  \n",
       "..     ...    ...        ...  \n",
       "575  10.20  25.35   7.420833  \n",
       "576   6.15  27.77  15.304167  \n",
       "577  17.17  32.50  13.537500  \n",
       "578   8.21  25.78   6.412500  \n",
       "579   8.37  25.99   9.929167  \n",
       "\n",
       "[577 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Nox_tropo'], axis=1).values\n",
    "y = df['Nox_tropo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "n_hours = 1\n",
    "n_features = 13\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours , 4)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]], axis=1, inplace=True)\n",
    "# print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)',\n",
       "       'var6(t-1)', 'var7(t-1)', 'var8(t-1)', 'var9(t-1)', 'var10(t-1)',\n",
       "       'var11(t-1)', 'var12(t-1)', 'var13(t-1)', 'var1(t+3)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 13) 460 (460,)\n",
      "(460, 1, 13) (460,) (113, 1, 13) (113,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "#80% training data\n",
    "n_train_hours = 460\n",
    "train = values[:n_train_hours]\n",
    "test = values[n_train_hours:]\n",
    "\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# train = data.values[:459]\n",
    "# test = data.values[459:]\n",
    "\n",
    "# # Separate input and output\n",
    "# train_X, train_y = train[:, :-1], train[:, -1]\n",
    "# test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# # Reshape input to be 3D [samples, timesteps, features]\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "# # Print all shapes\n",
    "# train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14313806, 0.18170149, 0.14215485, 0.16618872, 0.19475625,\n",
       "       0.13737539, 0.1449406 , 0.10468387, 0.10990031, 0.1327598 ,\n",
       "       0.2962174 , 0.24296054, 0.13729346, 0.3498293 , 0.3001229 ,\n",
       "       0.20092858, 0.20464291, 0.22698348, 0.16657108, 0.19117848,\n",
       "       0.14808139, 0.2341117 , 0.07936638, 0.29602622, 0.22714734,\n",
       "       0.20849379, 0.25874642, 0.10301789, 0.108644  , 0.17866994,\n",
       "       0.201229  , 0.15515499, 0.18735491, 0.16733579, 0.17017616,\n",
       "       0.2211935 , 0.21627748, 0.12265465, 0.1517684 , 0.19554827,\n",
       "       0.26210569, 0.12503073, 0.20546224, 0.16231053, 0.30725113,\n",
       "       0.27783695, 0.35821385, 0.27393145, 0.39912604, 0.17858801,\n",
       "       0.22335109, 0.17823296, 0.27368565, 0.17869726, 0.16501434,\n",
       "       0.18940325, 0.34297419, 0.19309026, 0.16741772, 0.22698348,\n",
       "       0.1809914 , 0.18153762, 0.20294961, 0.14488598, 0.18915745,\n",
       "       0.45647958, 0.58989485, 0.37072238, 0.42138468, 0.3415267 ,\n",
       "       0.50293596, 0.16476854, 0.20297692, 0.10301789, 0.33497201,\n",
       "       0.30132459, 0.35938823, 0.19063225, 0.32322819, 0.30686877,\n",
       "       0.40379626, 0.48586645, 0.10484774, 0.22255906, 0.23383859,\n",
       "       0.26005735, 0.25809095, 0.24888707, 0.57659429, 0.49755565,\n",
       "       0.26033047, 0.34570531, 0.25361191, 0.16195548, 0.14925577,\n",
       "       0.15938823, 0.17998088, 0.26541035, 0.24170422, 0.39038645,\n",
       "       0.31945924, 0.47100915, 0.27095453, 0.06978014, 0.54677045,\n",
       "       0.14682507, 0.25495016, 0.18678137, 0.11541718, 0.25241021,\n",
       "       0.26614775, 0.30449269, 0.29649051, 0.313997  , 0.13939642,\n",
       "       0.43820838, 0.30793391, 0.29236652, 0.26341663, 0.96621603,\n",
       "       0.39139697, 0.37640311, 0.33092995, 0.39792435, 0.20581729,\n",
       "       0.20955892, 0.21272702, 0.32858118, 0.25915608, 0.21854431,\n",
       "       0.27278438, 0.45817288, 0.26278847, 0.13103919, 0.1639492 ,\n",
       "       0.08878875, 0.2353134 , 0.54955619, 0.10886249, 0.26429059,\n",
       "       0.5571214 , 0.38118258, 0.08728663, 0.30949065, 0.2779462 ,\n",
       "       0.17550184, 0.22780281, 0.22911375, 0.6784378 , 0.40464291,\n",
       "       0.39145159, 0.14622423, 0.1213164 , 0.11467978, 0.12718831,\n",
       "       0.21466612, 0.27313942, 0.3193773 , 0.16703537, 0.19991807,\n",
       "       0.16465929, 0.15504575, 0.1965861 , 0.38336747, 0.30583094,\n",
       "       0.07111839, 0.16433156, 0.25077154, 0.23080705, 0.26978014,\n",
       "       0.35037553, 0.07057217, 0.57277072, 0.56648914, 0.34898266,\n",
       "       0.4163321 , 0.73046566, 0.29599891, 0.19199782, 0.05697119,\n",
       "       0.24760344, 0.35671173, 0.57659429, 0.85582412, 0.37653967,\n",
       "       0.14483135, 0.34879148, 0.40944968, 0.26420866, 0.32503073,\n",
       "       0.22064728, 0.21040557, 0.20163867, 0.15903318, 0.36405845,\n",
       "       0.34671583, 0.28816059, 0.79109655, 0.51549911, 0.2830807 ,\n",
       "       0.25606992, 0.28758705, 0.26969821, 0.25232828, 0.29908507,\n",
       "       0.41600437, 0.30727844, 0.79948109, 1.        , 0.86155947,\n",
       "       0.2521371 , 0.19606719, 0.15261505, 0.21674177, 0.18705449,\n",
       "       0.22687423, 0.22695617, 0.12186262, 0.24926943, 0.17214256,\n",
       "       0.15220538, 0.09796531, 0.12309163, 0.05784515, 0.10143384,\n",
       "       0.09788338, 0.14084392, 0.10553052, 0.26860576, 0.25145432,\n",
       "       0.19527516, 0.16561519, 0.23170832, 0.20928581, 0.21133415,\n",
       "       0.29392326, 0.42223133, 0.38650826, 0.35482726, 0.54171788,\n",
       "       0.17684009, 0.03687014, 0.06224225, 0.32251809, 0.30148846,\n",
       "       0.43891848, 0.40904001, 0.27262051, 0.23443944, 0.13863171,\n",
       "       0.24047522, 0.18459648, 0.34633347, 0.56599754, 0.19451045,\n",
       "       0.08302608, 0.13906869, 0.26562884, 0.15307934, 0.23648778,\n",
       "       0.14540489, 0.16790933, 0.07906596, 0.17353544, 0.1919705 ,\n",
       "       0.24976103, 0.13456234, 0.34846374, 0.33994265, 0.34210023,\n",
       "       0.29946743, 0.2730848 , 0.07445036, 0.14764441, 0.08512905,\n",
       "       0.16266557, 0.15182302, 0.20398744, 0.17667623, 0.1371569 ,\n",
       "       0.13041103, 0.12680595, 0.12656015, 0.14321999, 0.14791752,\n",
       "       0.13338796, 0.23124403, 0.25178206, 0.2255906 , 0.19833402,\n",
       "       0.17815103, 0.11508944, 0.09173836, 0.03809914, 0.35548273,\n",
       "       0.13150348, 0.12576813, 0.08351768, 0.22545405, 0.09949474,\n",
       "       0.05765397, 0.15081251, 0.17487369, 0.13865902, 0.06926123,\n",
       "       0.07180117, 0.17785061, 0.10039601, 0.24670217, 0.10301789,\n",
       "       0.10178888, 0.18921207, 0.13581865, 0.18735491, 0.12636897,\n",
       "       0.24763075, 0.1593063 , 0.17836952, 0.2285129 , 0.10370067,\n",
       "       0.37208794, 0.17451864, 0.1727161 , 0.1851427 , 0.05404889,\n",
       "       0.17591151, 0.08499249, 0.14013382, 0.08267104, 0.26000273,\n",
       "       0.1727161 , 0.19617643, 0.18631708, 0.12079749, 0.20051891,\n",
       "       0.21021439, 0.21537621, 0.0867131 , 0.24386181, 0.09690018,\n",
       "       0.15993445, 0.1897583 , 0.18970367, 0.16766353, 0.27753653,\n",
       "       0.22821248, 0.36239246, 0.17460057, 0.14185443, 0.19377304,\n",
       "       0.17815103, 0.0911102 , 0.20505257, 0.18519732, 0.09520688,\n",
       "       0.13625563, 0.18669944, 0.14783559, 0.23550457, 0.14636078,\n",
       "       0.19795166, 0.18334016, 0.19336338, 0.17050389, 0.25544176,\n",
       "       0.18852929, 0.14548682, 0.25241021, 0.28100505, 0.3047385 ,\n",
       "       0.31697392, 0.14018845, 0.29474259, 0.21084255, 0.16547863,\n",
       "       0.20928581, 0.15777687, 0.18129182, 0.18899358, 0.14499522,\n",
       "       0.06439984, 0.20371432, 0.15441759, 0.19907142, 0.26808685,\n",
       "       0.27398607, 0.23252765, 0.30441076, 0.24375256, 0.31500751,\n",
       "       0.22900451, 0.2090127 , 0.40854841, 0.28816059, 0.46336201,\n",
       "       0.32582275, 0.29326779, 0.17470982, 0.26642087, 0.3264236 ,\n",
       "       0.52757067, 0.25393964, 0.3247303 , 0.47557012, 0.25718968,\n",
       "       0.1719787 , 0.19811553, 0.53652875, 0.69534344, 0.36302062,\n",
       "       0.39300833, 0.21280896, 0.14510447, 0.55135873, 0.70105148,\n",
       "       0.12590468, 0.40928581, 0.46565615, 0.47264782, 0.20546224,\n",
       "       0.26117711, 0.17673085, 0.20065547, 0.28835177, 0.23965588,\n",
       "       0.23842687, 0.23747098, 0.40046429, 0.2277755 , 0.18973098,\n",
       "       0.2684419 , 0.15654786, 0.30981838, 0.29862078, 0.24981565,\n",
       "       0.29094633, 0.27592517, 0.13024717, 0.20027311, 0.25440393,\n",
       "       0.21581319, 0.20284037, 0.25888297, 0.23023351, 0.22458009,\n",
       "       0.15157722, 0.26677591, 0.66677591, 0.67849242, 0.47106377,\n",
       "       0.24383449, 0.3705312 , 0.10146115, 0.13969685, 0.20079203,\n",
       "       0.1476171 , 0.14808139, 0.18110064, 0.08813328, 0.10411034])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape[0], train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 - 2s - loss: 0.0146 - val_loss: 0.0269\n",
      "Epoch 2/50\n",
      "154/154 - 0s - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 3/50\n",
      "154/154 - 0s - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 4/50\n",
      "154/154 - 0s - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 5/50\n",
      "154/154 - 0s - loss: 8.8435e-04 - val_loss: 0.0023\n",
      "Epoch 6/50\n",
      "154/154 - 0s - loss: 7.2948e-04 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "154/154 - 0s - loss: 5.8782e-04 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "154/154 - 0s - loss: 4.9040e-04 - val_loss: 0.0013\n",
      "Epoch 9/50\n",
      "154/154 - 0s - loss: 4.1302e-04 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "154/154 - 0s - loss: 3.5986e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "154/154 - 0s - loss: 3.2157e-04 - val_loss: 0.0010\n",
      "Epoch 12/50\n",
      "154/154 - 0s - loss: 2.8872e-04 - val_loss: 9.8012e-04\n",
      "Epoch 13/50\n",
      "154/154 - 0s - loss: 2.7002e-04 - val_loss: 9.3275e-04\n",
      "Epoch 14/50\n",
      "154/154 - 0s - loss: 2.5286e-04 - val_loss: 8.7474e-04\n",
      "Epoch 15/50\n",
      "154/154 - 0s - loss: 2.3644e-04 - val_loss: 8.0550e-04\n",
      "Epoch 16/50\n",
      "154/154 - 0s - loss: 2.2423e-04 - val_loss: 7.6634e-04\n",
      "Epoch 17/50\n",
      "154/154 - 0s - loss: 2.1072e-04 - val_loss: 7.2607e-04\n",
      "Epoch 18/50\n",
      "154/154 - 0s - loss: 1.9897e-04 - val_loss: 6.7889e-04\n",
      "Epoch 19/50\n",
      "154/154 - 0s - loss: 1.8951e-04 - val_loss: 6.4665e-04\n",
      "Epoch 20/50\n",
      "154/154 - 0s - loss: 1.8131e-04 - val_loss: 6.2979e-04\n",
      "Epoch 21/50\n",
      "154/154 - 0s - loss: 1.7157e-04 - val_loss: 6.4362e-04\n",
      "Epoch 22/50\n",
      "154/154 - 0s - loss: 1.6163e-04 - val_loss: 6.1202e-04\n",
      "Epoch 23/50\n",
      "154/154 - 0s - loss: 1.6150e-04 - val_loss: 6.2274e-04\n",
      "Epoch 24/50\n",
      "154/154 - 0s - loss: 1.5115e-04 - val_loss: 6.0004e-04\n",
      "Epoch 25/50\n",
      "154/154 - 0s - loss: 1.4442e-04 - val_loss: 6.1175e-04\n",
      "Epoch 26/50\n",
      "154/154 - 0s - loss: 1.3779e-04 - val_loss: 5.7932e-04\n",
      "Epoch 27/50\n",
      "154/154 - 0s - loss: 1.3044e-04 - val_loss: 5.5279e-04\n",
      "Epoch 28/50\n",
      "154/154 - 0s - loss: 1.2733e-04 - val_loss: 5.0608e-04\n",
      "Epoch 29/50\n",
      "154/154 - 0s - loss: 1.1937e-04 - val_loss: 4.6992e-04\n",
      "Epoch 30/50\n",
      "154/154 - 0s - loss: 1.1430e-04 - val_loss: 4.4687e-04\n",
      "Epoch 31/50\n",
      "154/154 - 0s - loss: 1.0847e-04 - val_loss: 3.9829e-04\n",
      "Epoch 32/50\n",
      "154/154 - 0s - loss: 1.0672e-04 - val_loss: 3.9056e-04\n",
      "Epoch 33/50\n",
      "154/154 - 0s - loss: 1.0194e-04 - val_loss: 3.3181e-04\n",
      "Epoch 34/50\n",
      "154/154 - 0s - loss: 9.6749e-05 - val_loss: 3.2491e-04\n",
      "Epoch 35/50\n",
      "154/154 - 0s - loss: 9.2191e-05 - val_loss: 2.9430e-04\n",
      "Epoch 36/50\n",
      "154/154 - 0s - loss: 9.6047e-05 - val_loss: 2.7087e-04\n",
      "Epoch 37/50\n",
      "154/154 - 0s - loss: 9.2692e-05 - val_loss: 2.5376e-04\n",
      "Epoch 38/50\n",
      "154/154 - 0s - loss: 8.8172e-05 - val_loss: 2.4490e-04\n",
      "Epoch 39/50\n",
      "154/154 - 0s - loss: 7.9681e-05 - val_loss: 2.0501e-04\n",
      "Epoch 40/50\n",
      "154/154 - 0s - loss: 9.1759e-05 - val_loss: 2.1012e-04\n",
      "Epoch 41/50\n",
      "154/154 - 0s - loss: 7.1392e-05 - val_loss: 1.8901e-04\n",
      "Epoch 42/50\n",
      "154/154 - 1s - loss: 7.4510e-05 - val_loss: 1.7359e-04\n",
      "Epoch 43/50\n",
      "154/154 - 0s - loss: 8.2113e-05 - val_loss: 1.7338e-04\n",
      "Epoch 44/50\n",
      "154/154 - 0s - loss: 7.4799e-05 - val_loss: 1.5723e-04\n",
      "Epoch 45/50\n",
      "154/154 - 0s - loss: 7.7278e-05 - val_loss: 1.5653e-04\n",
      "Epoch 46/50\n",
      "154/154 - 0s - loss: 6.8028e-05 - val_loss: 1.4962e-04\n",
      "Epoch 47/50\n",
      "154/154 - 0s - loss: 6.8866e-05 - val_loss: 1.4518e-04\n",
      "Epoch 48/50\n",
      "154/154 - 0s - loss: 6.1792e-05 - val_loss: 1.4180e-04\n",
      "Epoch 49/50\n",
      "154/154 - 0s - loss: 5.2822e-05 - val_loss: 1.4152e-04\n",
      "Epoch 50/50\n",
      "154/154 - 0s - loss: 5.1595e-05 - val_loss: 1.3372e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(13, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu'))\n",
    "model.add(Dense(38, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=3, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 7.408\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -12:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -12:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.360938271484982"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.108620178131467"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(inv_y,inv_yhat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13-38-1 Test RMSE: 62.006\n",
    "\n",
    "13-68-1 Test RMSE: 68.752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 61-1 Test RMSE: Test RMSE: 63.163\n",
    "\n",
    "13-40-1 Test RMSE: 67.366\n",
    "\n",
    "13-100-1 Test RMSE: 65.965\n",
    "\n",
    "13- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
